{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeticiaHms/data-collection-mba/blob/main/limpeza_transformacao_analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgyItshuBfe4",
        "outputId": "90b94041-e3f0-4980-e555-0d7ceb1d21ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gerando datasets de exemplo...\n",
            "✓ dados_vendas_raw.csv criado: 1050 registros\n",
            "✓ dados_sensores_raw.csv criado: 520 registros\n",
            "\n",
            "============================================================\n",
            "PROBLEMAS INSERIDOS - DADOS DE VENDAS:\n",
            "============================================================\n",
            "- Duplicatas: ~50 registros\n",
            "- Valores ausentes: 619 células\n",
            "- Inconsistências de formato em 'produto' e 'cliente'\n",
            "- Outliers em 'preco_unitario'\n",
            "- Variações em 'status'\n",
            "\n",
            "============================================================\n",
            "PROBLEMAS INSERIDOS - DADOS DE SENSORES:\n",
            "============================================================\n",
            "- Timestamps duplicados: ~36\n",
            "- Valores ausentes: 45\n",
            "- Outliers em leituras dos sensores\n",
            "- Inconsistências em 'localizacao'\n",
            "\n",
            "✓ Datasets prontos para os exercícios de pré-processamento!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Script 01: Geração de Dados de Exemplo para Aula\n",
        "Gera datasets com problemas comuns para exercícios práticos\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "\n",
        "# Configuração de seed para reprodutibilidade\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "def gerar_dados_vendas_problematicos(n_registros=10000):\n",
        "    \"\"\"\n",
        "    Gera dataset de vendas com problemas típicos:\n",
        "    - Valores ausentes\n",
        "    - Duplicatas\n",
        "    - Outliers\n",
        "    - Inconsistências de formato\n",
        "    - Erros de digitação\n",
        "    \"\"\"\n",
        "\n",
        "    # IDs (com algumas duplicatas)\n",
        "    ids = list(range(1, n_registros + 1))\n",
        "    # Adicionar duplicatas (5% dos registros)\n",
        "    n_duplicatas = int(n_registros * 0.05)\n",
        "    ids_duplicados = random.choices(ids[:n_registros//2], k=n_duplicatas)\n",
        "    ids.extend(ids_duplicados)\n",
        "\n",
        "    # Datas\n",
        "    data_inicio = datetime(2023, 1, 1)\n",
        "    datas = [data_inicio + timedelta(days=random.randint(0, 365))\n",
        "             for _ in range(len(ids))]\n",
        "\n",
        "    # Produtos\n",
        "    produtos = ['Notebook', 'Mouse', 'Teclado', 'Monitor', 'Webcam',\n",
        "                'Headset', 'SSD', 'RAM', 'Mousepad', 'Hub USB']\n",
        "    lista_produtos = [random.choice(produtos) for _ in range(len(ids))]\n",
        "\n",
        "    # Adicionar inconsistências (maiúsculas/minúsculas, espaços)\n",
        "    for i in random.sample(range(len(lista_produtos)), len(lista_produtos)//10):\n",
        "        lista_produtos[i] = lista_produtos[i].lower()\n",
        "    for i in random.sample(range(len(lista_produtos)), len(lista_produtos)//10):\n",
        "        lista_produtos[i] = ' ' + lista_produtos[i] + ' '\n",
        "\n",
        "    # Quantidades\n",
        "    quantidades = [random.randint(1, 10) for _ in range(len(ids))]\n",
        "\n",
        "    # Preços (com outliers)\n",
        "    precos_base = {\n",
        "        'Notebook': 3500, 'Mouse': 50, 'Teclado': 150, 'Monitor': 800,\n",
        "        'Webcam': 200, 'Headset': 180, 'SSD': 400, 'RAM': 300,\n",
        "        'Mousepad': 30, 'Hub USB': 80\n",
        "    }\n",
        "\n",
        "    precos = []\n",
        "    for produto in lista_produtos:\n",
        "        produto_limpo = produto.strip().title()\n",
        "        preco_base = precos_base.get(produto_limpo, 100)\n",
        "        # Variação normal de ±20%\n",
        "        preco = preco_base * random.uniform(0.8, 1.2)\n",
        "        precos.append(round(preco, 2))\n",
        "\n",
        "    # Adicionar outliers (2% dos preços)\n",
        "    for i in random.sample(range(len(precos)), int(len(precos) * 0.02)):\n",
        "        precos[i] = precos[i] * random.uniform(10, 50)  # Outliers muito altos\n",
        "\n",
        "    # Clientes (com variações de formato)\n",
        "    clientes = []\n",
        "    for _ in range(len(ids)):\n",
        "        nome = random.choice(['João Silva', 'Maria Santos', 'Pedro Oliveira',\n",
        "                             'Ana Costa', 'Carlos Souza', 'Juliana Lima',\n",
        "                             'Fernando Alves', 'Patricia Rocha'])\n",
        "        # Variações: maiúscula, minúscula, capitalizada\n",
        "        formato = random.choice(['normal', 'upper', 'lower'])\n",
        "        if formato == 'upper':\n",
        "            nome = nome.upper()\n",
        "        elif formato == 'lower':\n",
        "            nome = nome.lower()\n",
        "        clientes.append(nome)\n",
        "\n",
        "    # Status (com erros de digitação)\n",
        "    status_validos = ['Concluído', 'Pendente', 'Cancelado', 'Em Processamento']\n",
        "    status_com_erros = status_validos + ['Concluido', 'concluído', 'PENDENTE',\n",
        "                                          'cancelado', 'Em processamento']\n",
        "    status = [random.choice(status_com_erros) for _ in range(len(ids))]\n",
        "\n",
        "    # Criar DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'id_venda': ids,\n",
        "        'data_venda': datas,\n",
        "        'produto': lista_produtos,\n",
        "        'quantidade': quantidades,\n",
        "        'preco_unitario': precos,\n",
        "        'cliente': clientes,\n",
        "        'status': status\n",
        "    })\n",
        "\n",
        "    # Adicionar valores ausentes (10% em colunas aleatórias)\n",
        "    colunas_para_missing = ['quantidade', 'preco_unitario', 'cliente', 'status']\n",
        "    for coluna in colunas_para_missing:\n",
        "        indices_missing = random.sample(range(len(df)), int(len(df) * 0.1))\n",
        "        df.loc[indices_missing, coluna] = np.nan\n",
        "\n",
        "    # Calcular valor total\n",
        "    df['valor_total'] = df['quantidade'] * df['preco_unitario']\n",
        "\n",
        "    # Embaralhar registros\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def gerar_dados_sensores_iot(n_registros=500):\n",
        "    \"\"\"\n",
        "    Gera dados de sensores IoT com problemas:\n",
        "    - Leituras fora do range esperado\n",
        "    - Timestamps duplicados\n",
        "    - Gaps temporais\n",
        "    - Encodings diferentes\n",
        "    \"\"\"\n",
        "\n",
        "    sensor_ids = ['TEMP_01', 'TEMP_02', 'HUMID_01', 'PRESS_01', 'PRESS_02']\n",
        "\n",
        "    data_inicio = datetime(2024, 1, 1, 0, 0, 0)\n",
        "    dados = []\n",
        "\n",
        "    for _ in range(n_registros):\n",
        "        sensor_id = random.choice(sensor_ids)\n",
        "\n",
        "        # Timestamp com alguns duplicados\n",
        "        minutos = random.randint(0, 10000)\n",
        "        timestamp = data_inicio + timedelta(minutes=minutos)\n",
        "\n",
        "        # Valores baseados no tipo de sensor\n",
        "        if 'TEMP' in sensor_id:\n",
        "            # Temperatura: esperado 15-30°C, outliers 0-50°C\n",
        "            if random.random() < 0.95:\n",
        "                valor = random.uniform(15, 30)\n",
        "            else:\n",
        "                valor = random.uniform(-10, 50)  # Outlier\n",
        "        elif 'HUMID' in sensor_id:\n",
        "            # Umidade: esperado 30-80%, outliers 0-100%\n",
        "            if random.random() < 0.95:\n",
        "                valor = random.uniform(30, 80)\n",
        "            else:\n",
        "                valor = random.uniform(0, 100)\n",
        "        else:  # Pressão\n",
        "            # Pressão: esperado 980-1020 hPa\n",
        "            if random.random() < 0.95:\n",
        "                valor = random.uniform(980, 1020)\n",
        "            else:\n",
        "                valor = random.uniform(900, 1100)\n",
        "\n",
        "        # Localização com inconsistências\n",
        "        localizacoes = ['Sala A', 'sala a', 'SALA A', 'Sala B', 'Armazém 1',\n",
        "                       'Armazem 1', 'armazém 1']\n",
        "        localizacao = random.choice(localizacoes)\n",
        "\n",
        "        dados.append({\n",
        "            'timestamp': timestamp,\n",
        "            'sensor_id': sensor_id,\n",
        "            'valor': round(valor, 2),\n",
        "            'localizacao': localizacao,\n",
        "            'unidade': 'C' if 'TEMP' in sensor_id else '%' if 'HUMID' in sensor_id else 'hPa'\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(dados)\n",
        "\n",
        "    # Adicionar valores ausentes\n",
        "    indices_missing = random.sample(range(len(df)), int(len(df) * 0.08))\n",
        "    df.loc[indices_missing, 'valor'] = np.nan\n",
        "\n",
        "    # Adicionar alguns registros com timestamp duplicado\n",
        "    for _ in range(20):\n",
        "        idx = random.randint(0, len(df)-1)\n",
        "        df = pd.concat([df, df.iloc[[idx]]], ignore_index=True)\n",
        "\n",
        "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Gerar datasets\n",
        "    print(\"Gerando datasets de exemplo...\")\n",
        "\n",
        "    # Dataset de vendas\n",
        "    df_vendas = gerar_dados_vendas_problematicos(1000)\n",
        "    df_vendas.to_csv('dados_vendas_raw.csv', index=False)\n",
        "    print(f\"✓ dados_vendas_raw.csv criado: {len(df_vendas)} registros\")\n",
        "\n",
        "    # Dataset de sensores IoT\n",
        "    df_sensores = gerar_dados_sensores_iot(500)\n",
        "    df_sensores.to_csv('dados_sensores_raw.csv', index=False)\n",
        "    print(f\"✓ dados_sensores_raw.csv criado: {len(df_sensores)} registros\")\n",
        "\n",
        "    # Exibir resumo dos problemas\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PROBLEMAS INSERIDOS - DADOS DE VENDAS:\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"- Duplicatas: ~{df_vendas['id_venda'].duplicated().sum()} registros\")\n",
        "    print(f\"- Valores ausentes: {df_vendas.isnull().sum().sum()} células\")\n",
        "    print(f\"- Inconsistências de formato em 'produto' e 'cliente'\")\n",
        "    print(f\"- Outliers em 'preco_unitario'\")\n",
        "    print(f\"- Variações em 'status'\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PROBLEMAS INSERIDOS - DADOS DE SENSORES:\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"- Timestamps duplicados: ~{df_sensores['timestamp'].duplicated().sum()}\")\n",
        "    print(f\"- Valores ausentes: {df_sensores['valor'].isnull().sum()}\")\n",
        "    print(f\"- Outliers em leituras dos sensores\")\n",
        "    print(f\"- Inconsistências em 'localizacao'\")\n",
        "\n",
        "    print(\"\\n✓ Datasets prontos para os exercícios de pré-processamento!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Script 02: Exploração Inicial e Diagnóstico de Qualidade\n",
        "Técnicas para identificar problemas nos dados\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurar estilo dos gráficos\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "\n",
        "def analise_completa_dataset(df, nome_dataset=\"Dataset\"):\n",
        "    \"\"\"\n",
        "    Realiza análise exploratória completa do dataset\n",
        "    \"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(f\"ANÁLISE DE QUALIDADE: {nome_dataset}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # 1. Informações Básicas\n",
        "    print(\"\\n1. INFORMAÇÕES BÁSICAS\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"Dimensões: {df.shape[0]} linhas x {df.shape[1]} colunas\")\n",
        "    print(f\"Memória utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "    # 2. Tipos de Dados\n",
        "    print(\"\\n2. TIPOS DE DADOS\")\n",
        "    print(\"-\" * 70)\n",
        "    print(df.dtypes)\n",
        "\n",
        "    print(\"\\nResumo de tipos:\")\n",
        "    for dtype, count in df.dtypes.value_counts().items():\n",
        "        print(f\"  {dtype}: {count} colunas\")\n",
        "\n",
        "    # 3. Valores Ausentes\n",
        "    print(\"\\n3. ANÁLISE DE VALORES AUSENTES\")\n",
        "    print(\"-\" * 70)\n",
        "    missing = df.isnull().sum()\n",
        "    missing_percent = 100 * missing / len(df)\n",
        "    missing_df = pd.DataFrame({\n",
        "        'Coluna': missing.index,\n",
        "        'Missing': missing.values,\n",
        "        'Percentual': missing_percent.values\n",
        "    })\n",
        "    missing_df = missing_df[missing_df['Missing'] > 0].sort_values('Missing', ascending=False)\n",
        "\n",
        "    if len(missing_df) > 0:\n",
        "        print(missing_df.to_string(index=False))\n",
        "        print(f\"\\nTotal de células com missing: {missing.sum():,}\")\n",
        "        print(f\"Percentual geral de missing: {100 * missing.sum() / (df.shape[0] * df.shape[1]):.2f}%\")\n",
        "    else:\n",
        "        print(\"✓ Nenhum valor ausente encontrado\")\n",
        "\n",
        "    # 4. Duplicatas\n",
        "    print(\"\\n4. ANÁLISE DE DUPLICATAS\")\n",
        "    print(\"-\" * 70)\n",
        "    duplicatas_completas = df.duplicated().sum()\n",
        "    print(f\"Linhas completamente duplicadas: {duplicatas_completas}\")\n",
        "\n",
        "    if duplicatas_completas > 0:\n",
        "        print(f\"Percentual de duplicatas: {100 * duplicatas_completas / len(df):.2f}%\")\n",
        "        print(\"\\nExemplo de registros duplicados (primeiros 4):\")\n",
        "        print(df[df.duplicated(keep=False)].head(4))\n",
        "\n",
        "    # 5. Estatísticas Descritivas\n",
        "    print(\"\\n5. ESTATÍSTICAS DESCRITIVAS\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    # Numéricas\n",
        "    colunas_numericas = df.select_dtypes(include=[np.number]).columns\n",
        "    if len(colunas_numericas) > 0:\n",
        "        print(\"\\nVariáveis Numéricas:\")\n",
        "        print(df[colunas_numericas].describe())\n",
        "\n",
        "    # Categóricas\n",
        "    colunas_categoricas = df.select_dtypes(include=['object']).columns\n",
        "    if len(colunas_categoricas) > 0:\n",
        "        print(\"\\nVariáveis Categóricas:\")\n",
        "        for col in colunas_categoricas:\n",
        "            valores_unicos = df[col].nunique()\n",
        "            print(f\"\\n{col}: {valores_unicos} valores únicos\")\n",
        "            if valores_unicos <= 20:\n",
        "                print(df[col].value_counts().head(10))\n",
        "            else:\n",
        "                print(f\"(Muitos valores únicos, mostrando top 5)\")\n",
        "                print(df[col].value_counts().head(5))\n",
        "\n",
        "    # 6. Detecção de Outliers (método IQR)\n",
        "    print(\"\\n6. DETECÇÃO DE OUTLIERS (Método IQR)\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for col in colunas_numericas:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
        "\n",
        "        if len(outliers) > 0:\n",
        "            print(f\"\\n{col}:\")\n",
        "            print(f\"  Range esperado: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
        "            print(f\"  Outliers encontrados: {len(outliers)} ({100*len(outliers)/len(df):.2f}%)\")\n",
        "            print(f\"  Valor mínimo outlier: {outliers.min():.2f}\")\n",
        "            print(f\"  Valor máximo outlier: {outliers.max():.2f}\")\n",
        "\n",
        "    return missing_df, duplicatas_completas\n",
        "\n",
        "\n",
        "def identificar_inconsistencias_categoricas(df, coluna):\n",
        "    \"\"\"\n",
        "    Identifica possíveis inconsistências em colunas categóricas\n",
        "    (variações de capitalização, espaços, etc.)\n",
        "    \"\"\"\n",
        "    print(f\"\\nANÁLISE DE INCONSISTÊNCIAS: {coluna}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    if coluna not in df.columns:\n",
        "        print(f\"✗ Coluna '{coluna}' não encontrada no dataset\")\n",
        "        return {}\n",
        "\n",
        "    # Valores únicos\n",
        "    valores = df[coluna].dropna().unique()\n",
        "    print(f\"Total de valores únicos: {len(valores)}\")\n",
        "\n",
        "    # Agrupar por versão normalizada\n",
        "    normalizados = {}\n",
        "    for valor in valores:\n",
        "        chave = str(valor).strip().lower()\n",
        "        if chave not in normalizados:\n",
        "            normalizados[chave] = []\n",
        "        normalizados[chave].append(valor)\n",
        "\n",
        "    # Identificar grupos com variações\n",
        "    print(\"\\nGrupos com variações de formato:\")\n",
        "    inconsistencias_encontradas = False\n",
        "    for chave, variacoes in normalizados.items():\n",
        "        if len(variacoes) > 1:\n",
        "            inconsistencias_encontradas = True\n",
        "            print(f\"\\n'{chave}' tem {len(variacoes)} variações:\")\n",
        "            for v in variacoes:\n",
        "                count = df[df[coluna] == v].shape[0]\n",
        "                print(f\"  - '{v}': {count} ocorrências\")\n",
        "\n",
        "    if not inconsistencias_encontradas:\n",
        "        print(\"✓ Nenhuma inconsistência de formato detectada\")\n",
        "\n",
        "    return normalizados\n",
        "\n",
        "\n",
        "def visualizar_distribuicoes(df, colunas_numericas=None, figsize=(15, 10)):\n",
        "    \"\"\"\n",
        "    Cria visualizações para análise de distribuições e outliers\n",
        "    \"\"\"\n",
        "    if colunas_numericas is None:\n",
        "        colunas_numericas = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "    if len(colunas_numericas) == 0:\n",
        "        print(\"Nenhuma coluna numérica para visualizar\")\n",
        "        return\n",
        "\n",
        "    n_cols = min(3, len(colunas_numericas))\n",
        "    n_rows = (len(colunas_numericas) + n_cols - 1) // n_cols\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
        "\n",
        "    # Garantir que axes seja sempre um array\n",
        "    if n_rows == 1 and n_cols == 1:\n",
        "        axes = np.array([axes])\n",
        "    elif n_rows == 1 or n_cols == 1:\n",
        "        axes = axes.flatten()\n",
        "    else:\n",
        "        axes = axes.flatten()\n",
        "\n",
        "    for idx, col in enumerate(colunas_numericas):\n",
        "        ax = axes[idx]\n",
        "\n",
        "        # Remover NaN para visualização\n",
        "        data_clean = df[col].dropna()\n",
        "\n",
        "        if len(data_clean) == 0:\n",
        "            ax.text(0.5, 0.5, f'{col}\\n(Sem dados)',\n",
        "                   ha='center', va='center', fontsize=12)\n",
        "            ax.set_xticks([])\n",
        "            ax.set_yticks([])\n",
        "            continue\n",
        "\n",
        "        # Histograma\n",
        "        ax.hist(data_clean, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        ax.set_xlabel(col)\n",
        "        ax.set_ylabel('Frequência')\n",
        "        ax.set_title(f'Distribuição: {col}')\n",
        "\n",
        "        # Adicionar linha vertical para média e mediana\n",
        "        media = data_clean.mean()\n",
        "        mediana = data_clean.median()\n",
        "        ax.axvline(media, color='red', linestyle='--', linewidth=2, label=f'Média: {media:.2f}')\n",
        "        ax.axvline(mediana, color='green', linestyle='--', linewidth=2, label=f'Mediana: {mediana:.2f}')\n",
        "        ax.legend(fontsize=8)\n",
        "\n",
        "        # Grid\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Remover subplots extras\n",
        "    for idx in range(len(colunas_numericas), len(axes)):\n",
        "        fig.delaxes(axes[idx])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('distribuicoes_dados.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"\\n✓ Gráfico salvo: distribuicoes_dados.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def visualizar_boxplots(df, colunas_numericas=None):\n",
        "    \"\"\"\n",
        "    Cria boxplots para identificação visual de outliers\n",
        "    \"\"\"\n",
        "    if colunas_numericas is None:\n",
        "        colunas_numericas = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "    if len(colunas_numericas) == 0:\n",
        "        print(\"Nenhuma coluna numérica para visualizar\")\n",
        "        return\n",
        "\n",
        "    n_cols = min(3, len(colunas_numericas))\n",
        "    n_rows = (len(colunas_numericas) + n_cols - 1) // n_cols\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 3))\n",
        "\n",
        "    # Garantir que axes seja sempre um array\n",
        "    if n_rows == 1 and n_cols == 1:\n",
        "        axes = np.array([axes])\n",
        "    elif n_rows == 1 or n_cols == 1:\n",
        "        axes = axes.flatten()\n",
        "    else:\n",
        "        axes = axes.flatten()\n",
        "\n",
        "    for idx, col in enumerate(colunas_numericas):\n",
        "        ax = axes[idx]\n",
        "\n",
        "        # Remover NaN\n",
        "        data_clean = df[col].dropna()\n",
        "\n",
        "        if len(data_clean) == 0:\n",
        "            ax.text(0.5, 0.5, f'{col}\\n(Sem dados)',\n",
        "                   ha='center', va='center', fontsize=12)\n",
        "            ax.set_xticks([])\n",
        "            ax.set_yticks([])\n",
        "            continue\n",
        "\n",
        "        # Boxplot\n",
        "        bp = ax.boxplot(data_clean, vert=True, patch_artist=True)\n",
        "        bp['boxes'][0].set_facecolor('lightblue')\n",
        "        bp['boxes'][0].set_edgecolor('black')\n",
        "\n",
        "        ax.set_ylabel('Valores')\n",
        "        ax.set_title(f'Boxplot: {col}')\n",
        "        ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Remover subplots extras\n",
        "    for idx in range(len(colunas_numericas), len(axes)):\n",
        "        fig.delaxes(axes[idx])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('boxplots_dados.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"✓ Gráfico salvo: boxplots_dados.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def visualizar_missing_data(df):\n",
        "    \"\"\"\n",
        "    Cria visualização de dados ausentes\n",
        "    \"\"\"\n",
        "    missing_data = df.isnull().sum()\n",
        "    missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
        "\n",
        "    if len(missing_data) == 0:\n",
        "        print(\"\\n✓ Nenhum valor ausente para visualizar\")\n",
        "        return\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Gráfico de barras\n",
        "    missing_data.plot(kind='bar', ax=ax1, color='coral', edgecolor='black')\n",
        "    ax1.set_title('Valores Ausentes por Coluna (Contagem)', fontsize=14, fontweight='bold')\n",
        "    ax1.set_ylabel('Número de Valores Ausentes')\n",
        "    ax1.set_xlabel('Colunas')\n",
        "    ax1.grid(True, alpha=0.3, axis='y')\n",
        "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "\n",
        "    # Gráfico de percentual\n",
        "    missing_percent = (missing_data / len(df)) * 100\n",
        "    missing_percent.plot(kind='bar', ax=ax2, color='salmon', edgecolor='black')\n",
        "    ax2.set_title('Valores Ausentes por Coluna (Percentual)', fontsize=14, fontweight='bold')\n",
        "    ax2.set_ylabel('Percentual (%)')\n",
        "    ax2.set_xlabel('Colunas')\n",
        "    ax2.grid(True, alpha=0.3, axis='y')\n",
        "    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('missing_data_visualization.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"\\n✓ Gráfico salvo: missing_data_visualization.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def visualizar_correlacoes(df):\n",
        "    \"\"\"\n",
        "    Cria heatmap de correlação entre variáveis numéricas\n",
        "    \"\"\"\n",
        "    colunas_numericas = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "    if len(colunas_numericas) < 2:\n",
        "        print(\"\\nInsuficientes colunas numéricas para matriz de correlação\")\n",
        "        return\n",
        "\n",
        "    # Calcular correlação\n",
        "    corr_matrix = df[colunas_numericas].corr()\n",
        "\n",
        "    # Criar heatmap\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "                fmt='.2f', square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "    plt.title('Matriz de Correlação', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"\\n✓ Gráfico salvo: correlation_matrix.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def gerar_relatorio_qualidade(df, nome_arquivo='relatorio_qualidade.txt'):\n",
        "    \"\"\"\n",
        "    Gera relatório textual completo de qualidade dos dados\n",
        "    \"\"\"\n",
        "    with open(nome_arquivo, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"=\"*70 + \"\\n\")\n",
        "        f.write(\"RELATÓRIO DE QUALIDADE DE DADOS\\n\")\n",
        "        f.write(f\"Gerado em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(\"=\"*70 + \"\\n\\n\")\n",
        "\n",
        "        # Completude\n",
        "        f.write(\"1. COMPLETUDE\\n\")\n",
        "        f.write(\"-\"*70 + \"\\n\")\n",
        "        total_cells = df.shape[0] * df.shape[1]\n",
        "        missing_cells = df.isnull().sum().sum()\n",
        "        completude = 100 * (1 - missing_cells / total_cells)\n",
        "        f.write(f\"Completude geral: {completude:.2f}%\\n\")\n",
        "        f.write(f\"Células com dados: {total_cells - missing_cells:,}\\n\")\n",
        "        f.write(f\"Células ausentes: {missing_cells:,}\\n\\n\")\n",
        "\n",
        "        # Completude por coluna\n",
        "        f.write(\"Completude por coluna:\\n\")\n",
        "        for col in df.columns:\n",
        "            missing = df[col].isnull().sum()\n",
        "            comp = 100 * (1 - missing / len(df))\n",
        "            f.write(f\"  {col}: {comp:.2f}%\\n\")\n",
        "\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "        # Unicidade\n",
        "        f.write(\"2. UNICIDADE\\n\")\n",
        "        f.write(\"-\"*70 + \"\\n\")\n",
        "        duplicatas = df.duplicated().sum()\n",
        "        unicidade = 100 * (1 - duplicatas / len(df))\n",
        "        f.write(f\"Unicidade: {unicidade:.2f}%\\n\")\n",
        "        f.write(f\"Registros únicos: {len(df) - duplicatas:,}\\n\")\n",
        "        f.write(f\"Registros duplicados: {duplicatas:,}\\n\\n\")\n",
        "\n",
        "        # Consistência\n",
        "        f.write(\"3. CONSISTÊNCIA\\n\")\n",
        "        f.write(\"-\"*70 + \"\\n\")\n",
        "\n",
        "        # Verificar tipos de dados\n",
        "        f.write(\"Tipos de dados:\\n\")\n",
        "        for col, dtype in df.dtypes.items():\n",
        "            f.write(f\"  {col}: {dtype}\\n\")\n",
        "\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "        # Validade (ranges)\n",
        "        f.write(\"4. VALIDADE - ESTATÍSTICAS\\n\")\n",
        "        f.write(\"-\"*70 + \"\\n\")\n",
        "        colunas_numericas = df.select_dtypes(include=[np.number]).columns\n",
        "        for col in colunas_numericas:\n",
        "            f.write(f\"\\n{col}:\\n\")\n",
        "            f.write(f\"  Mínimo: {df[col].min()}\\n\")\n",
        "            f.write(f\"  Máximo: {df[col].max()}\\n\")\n",
        "            f.write(f\"  Média: {df[col].mean():.2f}\\n\")\n",
        "            f.write(f\"  Mediana: {df[col].median():.2f}\\n\")\n",
        "            f.write(f\"  Desvio padrão: {df[col].std():.2f}\\n\")\n",
        "\n",
        "    print(f\"\\n✓ Relatório salvo: {nome_arquivo}\")\n",
        "\n",
        "\n",
        "def gerar_relatorio_html(df, nome_arquivo='relatorio_exploracao.html'):\n",
        "    \"\"\"\n",
        "    Gera relatório HTML interativo\n",
        "    \"\"\"\n",
        "    html_content = f\"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>Relatório de Exploração de Dados</title>\n",
        "        <style>\n",
        "            body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}\n",
        "            h1 {{ color: #333; border-bottom: 3px solid #4CAF50; padding-bottom: 10px; }}\n",
        "            h2 {{ color: #555; margin-top: 30px; }}\n",
        "            table {{ border-collapse: collapse; width: 100%; margin-top: 20px; background-color: white; }}\n",
        "            th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}\n",
        "            th {{ background-color: #4CAF50; color: white; }}\n",
        "            tr:nth-child(even) {{ background-color: #f2f2f2; }}\n",
        "            .metric {{ display: inline-block; margin: 10px; padding: 15px; background-color: white;\n",
        "                     border-radius: 5px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}\n",
        "            .metric-value {{ font-size: 24px; font-weight: bold; color: #4CAF50; }}\n",
        "            .metric-label {{ font-size: 14px; color: #666; }}\n",
        "            .warning {{ color: #ff9800; font-weight: bold; }}\n",
        "            .error {{ color: #f44336; font-weight: bold; }}\n",
        "            .success {{ color: #4CAF50; font-weight: bold; }}\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>Relatório de Exploração de Dados</h1>\n",
        "        <p><strong>Gerado em:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
        "\n",
        "        <h2>Métricas Principais</h2>\n",
        "        <div class=\"metric\">\n",
        "            <div class=\"metric-value\">{df.shape[0]:,}</div>\n",
        "            <div class=\"metric-label\">Linhas</div>\n",
        "        </div>\n",
        "        <div class=\"metric\">\n",
        "            <div class=\"metric-value\">{df.shape[1]}</div>\n",
        "            <div class=\"metric-label\">Colunas</div>\n",
        "        </div>\n",
        "        <div class=\"metric\">\n",
        "            <div class=\"metric-value\">{100 * (1 - df.isnull().sum().sum() / (df.shape[0] * df.shape[1])):.1f}%</div>\n",
        "            <div class=\"metric-label\">Completude</div>\n",
        "        </div>\n",
        "        <div class=\"metric\">\n",
        "            <div class=\"metric-value\">{df.duplicated().sum():,}</div>\n",
        "            <div class=\"metric-label\">Duplicatas</div>\n",
        "        </div>\n",
        "\n",
        "        <h2>Estrutura dos Dados</h2>\n",
        "        <table>\n",
        "            <tr>\n",
        "                <th>Coluna</th>\n",
        "                <th>Tipo</th>\n",
        "                <th>Valores Únicos</th>\n",
        "                <th>Missing</th>\n",
        "                <th>Completude</th>\n",
        "            </tr>\n",
        "    \"\"\"\n",
        "\n",
        "    for col in df.columns:\n",
        "        dtype = df[col].dtype\n",
        "        nunique = df[col].nunique()\n",
        "        missing = df[col].isnull().sum()\n",
        "        completude = 100 * (1 - missing / len(df))\n",
        "\n",
        "        completude_class = 'success' if completude > 95 else 'warning' if completude > 80 else 'error'\n",
        "\n",
        "        html_content += f\"\"\"\n",
        "            <tr>\n",
        "                <td><strong>{col}</strong></td>\n",
        "                <td>{dtype}</td>\n",
        "                <td>{nunique:,}</td>\n",
        "                <td>{missing:,}</td>\n",
        "                <td class=\"{completude_class}\">{completude:.1f}%</td>\n",
        "            </tr>\n",
        "        \"\"\"\n",
        "\n",
        "    html_content += \"\"\"\n",
        "        </table>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    with open(nome_arquivo, 'w', encoding='utf-8') as f:\n",
        "        f.write(html_content)\n",
        "\n",
        "    print(f\"\\n✓ Relatório HTML salvo: {nome_arquivo}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*70)\n",
        "    print(\"EXPLORAÇÃO INICIAL DE DADOS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Carregar dados de vendas\n",
        "    try:\n",
        "        print(\"\\nCarregando dados de vendas...\")\n",
        "        df_vendas = pd.read_csv('dados_vendas_raw.csv')\n",
        "        print(f\"✓ Arquivo carregado: {len(df_vendas)} registros\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\n⚠ Arquivo 'dados_vendas_raw.csv' não encontrado!\")\n",
        "        print(\"Execute primeiro o script 01_geracao_dados_exemplo.py\")\n",
        "        exit(1)\n",
        "\n",
        "    # Análise completa\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    missing_info, duplicatas = analise_completa_dataset(df_vendas, \"Vendas\")\n",
        "\n",
        "    # Análise de inconsistências em colunas específicas\n",
        "    if 'produto' in df_vendas.columns:\n",
        "        identificar_inconsistencias_categoricas(df_vendas, 'produto')\n",
        "\n",
        "    if 'cliente' in df_vendas.columns:\n",
        "        identificar_inconsistencias_categoricas(df_vendas, 'cliente')\n",
        "\n",
        "    if 'status' in df_vendas.columns:\n",
        "        identificar_inconsistencias_categoricas(df_vendas, 'status')\n",
        "\n",
        "    # Visualizações\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"GERANDO VISUALIZAÇÕES\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    colunas_para_visualizar = ['quantidade', 'preco_unitario', 'valor_total']\n",
        "    # Filtrar apenas colunas que existem\n",
        "    colunas_existentes = [col for col in colunas_para_visualizar if col in df_vendas.columns]\n",
        "\n",
        "    if colunas_existentes:\n",
        "        visualizar_distribuicoes(df_vendas, colunas_existentes)\n",
        "        visualizar_boxplots(df_vendas, colunas_existentes)\n",
        "\n",
        "    visualizar_missing_data(df_vendas)\n",
        "    visualizar_correlacoes(df_vendas)\n",
        "\n",
        "    # Gerar relatórios\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"GERANDO RELATÓRIOS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    gerar_relatorio_qualidade(df_vendas)\n",
        "    gerar_relatorio_html(df_vendas)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"EXPLORAÇÃO INICIAL CONCLUÍDA!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nArquivos gerados:\")\n",
        "    print(\"  • distribuicoes_dados.png\")\n",
        "    print(\"  • boxplots_dados.png\")\n",
        "    print(\"  • missing_data_visualization.png\")\n",
        "    print(\"  • correlation_matrix.png\")\n",
        "    print(\"  • relatorio_qualidade.txt\")\n",
        "    print(\"  • relatorio_exploracao.html\")\n",
        "    print(\"\\n✓ Revise os arquivos gerados para identificar problemas nos dados\")"
      ],
      "metadata": {
        "id": "tpnHa0_sCj_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Script 03: Técnicas de Limpeza de Dados\n",
        "Tratamento de valores ausentes, duplicatas, outliers e inconsistências\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "class LimpadorDados:\n",
        "    \"\"\"\n",
        "    Classe para aplicar técnicas de limpeza de dados\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df):\n",
        "        self.df = df.copy()\n",
        "        self.df_original = df.copy()\n",
        "        self.log_acoes = []\n",
        "\n",
        "    def registrar_acao(self, acao):\n",
        "        \"\"\"Registra ações de limpeza realizadas\"\"\"\n",
        "        self.log_acoes.append(acao)\n",
        "        print(f\"✓ {acao}\")\n",
        "\n",
        "    # ==================== TRATAMENTO DE VALORES AUSENTES ====================\n",
        "\n",
        "    def remover_linhas_com_missing(self, threshold=0.5):\n",
        "        \"\"\"\n",
        "        Remove linhas onde mais de 'threshold' das colunas têm valores ausentes\n",
        "        threshold: proporção de missing aceita (0 a 1)\n",
        "        \"\"\"\n",
        "        linhas_antes = len(self.df)\n",
        "        missing_por_linha = self.df.isnull().sum(axis=1) / len(self.df.columns)\n",
        "        self.df = self.df[missing_por_linha <= threshold]\n",
        "        linhas_removidas = linhas_antes - len(self.df)\n",
        "\n",
        "        self.registrar_acao(\n",
        "            f\"Removidas {linhas_removidas} linhas com >{threshold*100}% de missing\"\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def remover_colunas_com_missing(self, threshold=0.5):\n",
        "        \"\"\"\n",
        "        Remove colunas onde mais de 'threshold' dos valores são ausentes\n",
        "        \"\"\"\n",
        "        colunas_antes = len(self.df.columns)\n",
        "        missing_por_coluna = self.df.isnull().sum() / len(self.df)\n",
        "        colunas_manter = missing_por_coluna[missing_por_coluna <= threshold].index\n",
        "        self.df = self.df[colunas_manter]\n",
        "        colunas_removidas = colunas_antes - len(self.df.columns)\n",
        "\n",
        "        self.registrar_acao(\n",
        "            f\"Removidas {colunas_removidas} colunas com >{threshold*100}% de missing\"\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def imputar_media(self, colunas):\n",
        "        \"\"\"Imputa valores ausentes com a média da coluna\"\"\"\n",
        "        for col in colunas:\n",
        "            if col in self.df.columns:\n",
        "                missing_antes = self.df[col].isnull().sum()\n",
        "                media = self.df[col].mean()\n",
        "                self.df[col].fillna(media, inplace=True)\n",
        "                self.registrar_acao(\n",
        "                    f\"Imputados {missing_antes} valores em '{col}' com média {media:.2f}\"\n",
        "                )\n",
        "        return self\n",
        "\n",
        "    def imputar_mediana(self, colunas):\n",
        "        \"\"\"Imputa valores ausentes com a mediana da coluna\"\"\"\n",
        "        for col in colunas:\n",
        "            if col in self.df.columns:\n",
        "                missing_antes = self.df[col].isnull().sum()\n",
        "                mediana = self.df[col].median()\n",
        "                self.df[col].fillna(mediana, inplace=True)\n",
        "                self.registrar_acao(\n",
        "                    f\"Imputados {missing_antes} valores em '{col}' com mediana {mediana:.2f}\"\n",
        "                )\n",
        "        return self\n",
        "\n",
        "    def imputar_moda(self, colunas):\n",
        "        \"\"\"Imputa valores ausentes com a moda (valor mais frequente)\"\"\"\n",
        "        for col in colunas:\n",
        "            if col in self.df.columns:\n",
        "                missing_antes = self.df[col].isnull().sum()\n",
        "                moda = self.df[col].mode()[0] if not self.df[col].mode().empty else None\n",
        "                if moda is not None:\n",
        "                    self.df[col].fillna(moda, inplace=True)\n",
        "                    self.registrar_acao(\n",
        "                        f\"Imputados {missing_antes} valores em '{col}' com moda '{moda}'\"\n",
        "                    )\n",
        "        return self\n",
        "\n",
        "    def imputar_forward_fill(self, colunas):\n",
        "        \"\"\"Propagação forward (útil para séries temporais)\"\"\"\n",
        "        for col in colunas:\n",
        "            if col in self.df.columns:\n",
        "                missing_antes = self.df[col].isnull().sum()\n",
        "                self.df[col].fillna(method='ffill', inplace=True)\n",
        "                missing_depois = self.df[col].isnull().sum()\n",
        "                self.registrar_acao(\n",
        "                    f\"Forward fill em '{col}': {missing_antes - missing_depois} valores preenchidos\"\n",
        "                )\n",
        "        return self\n",
        "\n",
        "    def imputar_backward_fill(self, colunas):\n",
        "        \"\"\"Propagação backward (útil para séries temporais)\"\"\"\n",
        "        for col in colunas:\n",
        "            if col in self.df.columns:\n",
        "                missing_antes = self.df[col].isnull().sum()\n",
        "                self.df[col].fillna(method='bfill', inplace=True)\n",
        "                missing_depois = self.df[col].isnull().sum()\n",
        "                self.registrar_acao(\n",
        "                    f\"Backward fill em '{col}': {missing_antes - missing_depois} valores preenchidos\"\n",
        "                )\n",
        "        return self\n",
        "\n",
        "    def imputar_valor_constante(self, coluna, valor):\n",
        "        \"\"\"Imputa com valor constante específico\"\"\"\n",
        "        if coluna in self.df.columns:\n",
        "            missing_antes = self.df[coluna].isnull().sum()\n",
        "            self.df[coluna].fillna(valor, inplace=True)\n",
        "            self.registrar_acao(\n",
        "                f\"Imputados {missing_antes} valores em '{coluna}' com constante '{valor}'\"\n",
        "            )\n",
        "        return self\n",
        "\n",
        "    # ==================== TRATAMENTO DE DUPLICATAS ====================\n",
        "\n",
        "    def remover_duplicatas_completas(self, keep='first'):\n",
        "        \"\"\"\n",
        "        Remove linhas completamente duplicadas\n",
        "        keep: 'first', 'last' ou False (remove todas)\n",
        "        \"\"\"\n",
        "        duplicatas_antes = self.df.duplicated().sum()\n",
        "        self.df.drop_duplicates(keep=keep, inplace=True)\n",
        "        self.registrar_acao(\n",
        "            f\"Removidas {duplicatas_antes} linhas duplicadas (keep='{keep}')\"\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def remover_duplicatas_por_chave(self, colunas_chave, keep='first'):\n",
        "        \"\"\"Remove duplicatas baseado em colunas específicas\"\"\"\n",
        "        duplicatas_antes = self.df.duplicated(subset=colunas_chave).sum()\n",
        "        self.df.drop_duplicates(subset=colunas_chave, keep=keep, inplace=True)\n",
        "        self.registrar_acao(\n",
        "            f\"Removidas {duplicatas_antes} duplicatas baseadas em {colunas_chave}\"\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    # ==================== TRATAMENTO DE OUTLIERS ====================\n",
        "\n",
        "    def remover_outliers_iqr(self, colunas, multiplicador=1.5):\n",
        "        \"\"\"\n",
        "        Remove outliers usando método IQR (Interquartile Range)\n",
        "        multiplicador: 1.5 (padrão) ou 3.0 (extremos)\n",
        "        \"\"\"\n",
        "        linhas_antes = len(self.df)\n",
        "\n",
        "        for col in colunas:\n",
        "            if col in self.df.columns:\n",
        "                Q1 = self.df[col].quantile(0.25)\n",
        "                Q3 = self.df[col].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - multiplicador * IQR\n",
        "                upper_bound = Q3 + multiplicador * IQR\n",
        "\n",
        "                self.df = self.df[\n",
        "                    (self.df[col] >= lower_bound) & (self.df[col] <= upper_bound)\n",
        "                ]\n",
        "\n",
        "        linhas_removidas = linhas_antes - len(self.df)\n",
        "        self.registrar_acao(\n",
        "            f\"Removidos {linhas_removidas} outliers (IQR) das colunas {colunas}\"\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def remover_outliers_zscore(self, colunas, threshold=3):\n",
        "        \"\"\"\n",
        "        Remove outliers usando Z-score\n",
        "        threshold: normalmente 3 (valores além de 3 desvios padrão)\n",
        "        \"\"\"\n",
        "        linhas_antes = len(self.df)\n",
        "\n",
        "        for col in colunas:\n",
        "            if col in self.df.columns:\n",
        "                z_scores = np.abs(stats.zscore(self.df[col].dropna()))\n",
        "                self.df = self.df[\n",
        "                    (np.abs(stats.zscore(self.df[col])) < threshold) |\n",
        "                    self.df[col].isnull()\n",
        "                ]\n",
        "\n",
        "        linhas_removidas = linhas_antes - len(self.df)\n",
        "        self.registrar_acao(\n",
        "            f\"Removidos {linhas_removidas} outliers (Z-score) das colunas {colunas}\"\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def winsorizar(self, colunas, limits=(0.05, 0.05)):\n",
        "        \"\"\"\n",
        "        Winsorização: substitui outliers pelos valores nos percentis\n",
        "        limits: (lower, upper) - ex: (0.05, 0.05) = 5% inferior e superior\n",
        "        \"\"\"\n",
        "        from scipy.stats.mstats import winsorize\n",
        "\n",
        "        for col in colunas:\n",
        "            if col in self.df.columns:\n",
        "                valores_antes = self.df[col].copy()\n",
        "                self.df[col] = winsorize(self.df[col].dropna(), limits=limits)\n",
        "                modificados = (valores_antes != self.df[col]).sum()\n",
        "                self.registrar_acao(\n",
        "                    f\"Winsorização em '{col}': {modificados} valores ajustados\"\n",
        "                )\n",
        "        return self\n",
        "\n",
        "    def cap_outliers(self, coluna, lower_percentile=0.01, upper_percentile=0.99):\n",
        "        \"\"\"\n",
        "        Limita valores aos percentis especificados\n",
        "        \"\"\"\n",
        "        if coluna in self.df.columns:\n",
        "            lower_bound = self.df[coluna].quantile(lower_percentile)\n",
        "            upper_bound = self.df[coluna].quantile(upper_percentile)\n",
        "\n",
        "            modificados = ((self.df[coluna] < lower_bound) |\n",
        "                          (self.df[coluna] > upper_bound)).sum()\n",
        "\n",
        "            self.df[coluna] = self.df[coluna].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "            self.registrar_acao(\n",
        "                f\"Capping em '{coluna}': {modificados} valores limitados a [{lower_bound:.2f}, {upper_bound:.2f}]\"\n",
        "            )\n",
        "        return self\n",
        "\n",
        "    # ==================== PADRONIZAÇÃO E CONSISTÊNCIA ====================\n",
        "\n",
        "    def padronizar_texto(self, colunas, metodo='title'):\n",
        "        \"\"\"\n",
        "        Padroniza formato de texto\n",
        "        metodo: 'lower', 'upper', 'title', 'strip'\n",
        "        \"\"\"\n",
        "        for col in colunas:\n",
        "            if col in self.df.columns:\n",
        "                if metodo == 'lower':\n",
        "                    self.df[col] = self.df[col].str.lower()\n",
        "                elif metodo == 'upper':\n",
        "                    self.df[col] = self.df[col].str.upper()\n",
        "                elif metodo == 'title':\n",
        "                    self.df[col] = self.df[col].str.title()\n",
        "                elif metodo == 'strip':\n",
        "                    self.df[col] = self.df[col].str.strip()\n",
        "\n",
        "                self.registrar_acao(\n",
        "                    f\"Padronização '{metodo}' aplicada em '{col}'\"\n",
        "                )\n",
        "        return self\n",
        "\n",
        "    def remover_espacos_extras(self, colunas):\n",
        "        \"\"\"Remove espaços em branco extras\"\"\"\n",
        "        for col in colunas:\n",
        "            if col in self.df.columns:\n",
        "                self.df[col] = self.df[col].str.strip()\n",
        "                self.df[col] = self.df[col].str.replace(r'\\s+', ' ', regex=True)\n",
        "                self.registrar_acao(f\"Espaços extras removidos em '{col}'\")\n",
        "        return self\n",
        "\n",
        "    def substituir_valores(self, coluna, mapeamento):\n",
        "        \"\"\"\n",
        "        Substitui valores usando dicionário de mapeamento\n",
        "        mapeamento: dict com {valor_antigo: valor_novo}\n",
        "        \"\"\"\n",
        "        if coluna in self.df.columns:\n",
        "            substituicoes = 0\n",
        "            for old_val, new_val in mapeamento.items():\n",
        "                count = (self.df[coluna] == old_val).sum()\n",
        "                substituicoes += count\n",
        "                self.df[coluna] = self.df[coluna].replace(old_val, new_val)\n",
        "\n",
        "            self.registrar_acao(\n",
        "                f\"{substituicoes} valores substituídos em '{coluna}'\"\n",
        "            )\n",
        "        return self\n",
        "\n",
        "    def normalizar_categorias(self, coluna):\n",
        "        \"\"\"\n",
        "        Normaliza categorias agrupando variações similares\n",
        "        \"\"\"\n",
        "        if coluna in self.df.columns:\n",
        "            # Criar versão normalizada\n",
        "            valores_normalizados = self.df[coluna].str.strip().str.lower()\n",
        "\n",
        "            # Mapear para versão mais comum de cada grupo\n",
        "            mapeamento = {}\n",
        "            for valor_norm in valores_normalizados.unique():\n",
        "                if pd.notna(valor_norm):\n",
        "                    # Pegar variações originais\n",
        "                    variacoes = self.df[\n",
        "                        valores_normalizados == valor_norm\n",
        "                    ][coluna].dropna().unique()\n",
        "\n",
        "                    # Escolher a mais frequente como padrão\n",
        "                    if len(variacoes) > 0:\n",
        "                        counts = self.df[coluna].value_counts()\n",
        "                        mais_comum = max(variacoes, key=lambda x: counts.get(x, 0))\n",
        "\n",
        "                        for variacao in variacoes:\n",
        "                            if variacao != mais_comum:\n",
        "                                mapeamento[variacao] = mais_comum\n",
        "\n",
        "            if mapeamento:\n",
        "                self.df[coluna] = self.df[coluna].replace(mapeamento)\n",
        "                self.registrar_acao(\n",
        "                    f\"Normalizadas {len(mapeamento)} variações em '{coluna}'\"\n",
        "                )\n",
        "        return self\n",
        "\n",
        "    # ==================== VALIDAÇÃO E CONVERSÃO DE TIPOS ====================\n",
        "\n",
        "    def converter_tipo(self, coluna, tipo_alvo):\n",
        "        \"\"\"\n",
        "        Converte tipo de dados da coluna\n",
        "        tipo_alvo: 'int', 'float', 'string', 'datetime', 'category'\n",
        "        \"\"\"\n",
        "        if coluna in self.df.columns:\n",
        "            try:\n",
        "                if tipo_alvo == 'int':\n",
        "                    self.df[coluna] = pd.to_numeric(self.df[coluna], errors='coerce').astype('Int64')\n",
        "                elif tipo_alvo == 'float':\n",
        "                    self.df[coluna] = pd.to_numeric(self.df[coluna], errors='coerce')\n",
        "                elif tipo_alvo == 'string':\n",
        "                    self.df[coluna] = self.df[coluna].astype(str)\n",
        "                elif tipo_alvo == 'datetime':\n",
        "                    self.df[coluna] = pd.to_datetime(self.df[coluna], errors='coerce')\n",
        "                elif tipo_alvo == 'category':\n",
        "                    self.df[coluna] = self.df[coluna].astype('category')\n",
        "\n",
        "                self.registrar_acao(f\"Coluna '{coluna}' convertida para {tipo_alvo}\")\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Erro ao converter '{coluna}' para {tipo_alvo}: {str(e)}\")\n",
        "        return self\n",
        "\n",
        "    def validar_range(self, coluna, min_val, max_val, acao='remover'):\n",
        "        \"\"\"\n",
        "        Valida se valores estão dentro do range esperado\n",
        "        acao: 'remover', 'clipar', 'marcar'\n",
        "        \"\"\"\n",
        "        if coluna in self.df.columns:\n",
        "            fora_range = (self.df[coluna] < min_val) | (self.df[coluna] > max_val)\n",
        "            count_fora = fora_range.sum()\n",
        "\n",
        "            if acao == 'remover':\n",
        "                self.df = self.df[~fora_range]\n",
        "                self.registrar_acao(\n",
        "                    f\"Removidos {count_fora} valores fora do range [{min_val}, {max_val}] em '{coluna}'\"\n",
        "                )\n",
        "            elif acao == 'clipar':\n",
        "                self.df[coluna] = self.df[coluna].clip(lower=min_val, upper=max_val)\n",
        "                self.registrar_acao(\n",
        "                    f\"Clipados {count_fora} valores para range [{min_val}, {max_val}] em '{coluna}'\"\n",
        "                )\n",
        "            elif acao == 'marcar':\n",
        "                self.df[f'{coluna}_valido'] = ~fora_range\n",
        "                self.registrar_acao(\n",
        "                    f\"Marcados {count_fora} valores fora do range em '{coluna}_valido'\"\n",
        "                )\n",
        "        return self\n",
        "\n",
        "    # ==================== UTILIDADES ====================\n",
        "\n",
        "    def resetar(self):\n",
        "        \"\"\"Reseta para o dataframe original\"\"\"\n",
        "        self.df = self.df_original.copy()\n",
        "        self.log_acoes = []\n",
        "        self.registrar_acao(\"Dataset resetado para estado original\")\n",
        "        return self\n",
        "\n",
        "    def obter_df(self):\n",
        "        \"\"\"Retorna o dataframe limpo\"\"\"\n",
        "        return self.df\n",
        "\n",
        "    def exibir_log(self):\n",
        "        \"\"\"Exibe todas as ações realizadas\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"LOG DE LIMPEZA\")\n",
        "        print(\"=\"*70)\n",
        "        for i, acao in enumerate(self.log_acoes, 1):\n",
        "            print(f\"{i}. {acao}\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "    def comparar_antes_depois(self):\n",
        "        \"\"\"Compara estatísticas antes e depois da limpeza\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"COMPARAÇÃO: ANTES vs DEPOIS DA LIMPEZA\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        print(f\"\\nNúmero de linhas:\")\n",
        "        print(f\"  Antes: {len(self.df_original):,}\")\n",
        "        print(f\"  Depois: {len(self.df):,}\")\n",
        "        print(f\"  Diferença: {len(self.df_original) - len(self.df):,} linhas removidas\")\n",
        "\n",
        "        print(f\"\\nValores ausentes:\")\n",
        "        print(f\"  Antes: {self.df_original.isnull().sum().sum():,}\")\n",
        "        print(f\"  Depois: {self.df.isnull().sum().sum():,}\")\n",
        "\n",
        "        print(f\"\\nDuplicatas:\")\n",
        "        print(f\"  Antes: {self.df_original.duplicated().sum():,}\")\n",
        "        print(f\"  Depois: {self.df.duplicated().sum():,}\")\n",
        "\n",
        "        print(f\"\\nMemória:\")\n",
        "        mem_antes = self.df_original.memory_usage(deep=True).sum() / 1024**2\n",
        "        mem_depois = self.df.memory_usage(deep=True).sum() / 1024**2\n",
        "        print(f\"  Antes: {mem_antes:.2f} MB\")\n",
        "        print(f\"  Depois: {mem_depois:.2f} MB\")\n",
        "        print(f\"  Economia: {mem_antes - mem_depois:.2f} MB ({100*(mem_antes-mem_depois)/mem_antes:.1f}%)\")\n",
        "\n",
        "\n",
        "# ==================== EXEMPLO DE USO ====================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Carregar dados\n",
        "    print(\"Carregando dados...\")\n",
        "    df = pd.read_csv('dados_vendas_raw.csv')\n",
        "\n",
        "    print(f\"Dataset original: {df.shape[0]} linhas x {df.shape[1]} colunas\")\n",
        "\n",
        "    # Criar instância do limpador\n",
        "    limpador = LimpadorDados(df)\n",
        "\n",
        "    # Pipeline de limpeza\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"INICIANDO PIPELINE DE LIMPEZA\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    df_limpo = (limpador\n",
        "        # 1. Remover duplicatas completas\n",
        "        .remover_duplicatas_completas(keep='first')\n",
        "\n",
        "        # 2. Padronizar textos\n",
        "        .padronizar_texto(['produto', 'cliente', 'status'], metodo='title')\n",
        "        .remover_espacos_extras(['produto', 'cliente', 'status'])\n",
        "\n",
        "        # 3. Normalizar categorias\n",
        "        .normalizar_categorias('status')\n",
        "\n",
        "        # 4. Tratar valores ausentes\n",
        "        .imputar_mediana(['quantidade'])\n",
        "        .imputar_media(['preco_unitario'])\n",
        "        .imputar_moda(['status'])\n",
        "\n",
        "        # 5. Tratar outliers\n",
        "        .cap_outliers('preco_unitario', lower_percentile=0.01, upper_percentile=0.99)\n",
        "\n",
        "        # 6. Validar ranges\n",
        "        .validar_range('quantidade', min_val=0, max_val=100, acao='clipar')\n",
        "\n",
        "        # 7. Converter tipos\n",
        "        .converter_tipo('data_venda', 'datetime')\n",
        "\n",
        "        .obter_df()\n",
        "    )\n",
        "\n",
        "    # Exibir resultados\n",
        "    limpador.exibir_log()\n",
        "    limpador.comparar_antes_depois()\n",
        "\n",
        "    # Salvar dados limpos\n",
        "    df_limpo.to_csv('dados_vendas_limpos.csv', index=False)\n",
        "    print(\"\\n✓ Dados limpos salvos em: dados_vendas_limpos.csv\")\n",
        "\n",
        "    # Exibir amostra\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"AMOSTRA DOS DADOS LIMPOS\")\n",
        "    print(\"=\"*70)\n",
        "    print(df_limpo.head(10))"
      ],
      "metadata": {
        "id": "MnqqxjHrC1O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Script 04: Transformação de Dados\n",
        "Normalização, encoding, feature engineering e agregação\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import (\n",
        "    StandardScaler, MinMaxScaler, RobustScaler,\n",
        "    LabelEncoder, OneHotEncoder\n",
        ")\n",
        "from sklearn.decomposition import PCA\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "class TransformadorDados:\n",
        "    \"\"\"\n",
        "    Classe para aplicar transformações em dados\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df):\n",
        "        self.df = df.copy()\n",
        "        self.scalers = {}\n",
        "        self.encoders = {}\n",
        "\n",
        "    # ==================== NORMALIZAÇÃO E ESCALONAMENTO ====================\n",
        "\n",
        "    def aplicar_min_max_scaling(self, colunas, feature_range=(0, 1)):\n",
        "        \"\"\"\n",
        "        Min-Max Scaling: escala valores para um range específico\n",
        "        Formula: X_scaled = (X - X_min) / (X_max - X_min)\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"MIN-MAX SCALING\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        for col in colunas:\n",
        "            if col in self.df.columns:\n",
        "                scaler = MinMaxScaler(feature_range=feature_range)\n",
        "                valores_originais = self.df[col].values.reshape(-1, 1)\n",
        "                valores_escalados = scaler.fit_transform(valores_originais)\n",
        "\n",
        "                self.df[f'{col}_minmax'] = valores_escalados\n",
        "                self.scalers[f'{col}_minmax'] = scaler\n",
        "\n",
        "                print(f\"✓ {col}:\")\n",
        "                print(f\"  Original: [{self.df[col].min():.2f}, {self.df[col].max():.2f}]\")\n",
        "                print(f\"  Escalado: [{self.df[f'{col}_minmax'].min():.2f}, {self.df[f'{col}_minmax'].max():.2f}]\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def aplicar_standardization(self, colunas):\n",
        "        \"\"\"\n",
        "        Standardization (Z-score): média 0 e desvio padrão 1\n",
        "        Formula: X_scaled = (X - mean) / std\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"STANDARDIZATION (Z-SCORE)\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        for col in colunas:\n",
        "            if col in self.df.columns:\n",
        "                scaler = StandardScaler()\n",
        "                valores_originais = self.df[col].values.reshape(-1, 1)\n",
        "                valores_escalados = scaler.fit_transform(valores_originais)\n",
        "\n",
        "                self.df[f'{col}_std'] = valores_escalados\n",
        "                self.scalers[f'{col}_std'] = scaler\n",
        "\n",
        "                print(f\"✓ {col}:\")\n",
        "                print(f\"  Média original: {self.df[col].mean():.2f}\")\n",
        "                print(f\"  Std original: {self.df[col].std():.2f}\")\n",
        "                print(f\"  Média escalada: {self.df[f'{col}_std'].mean():.4f}\")\n",
        "                print(f\"  Std escalada: {self.df[f'{col}_std'].std():.4f}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def aplicar_robust_scaling(self, colunas):\n",
        "        \"\"\"\n",
        "        Robust Scaling: usa mediana e IQR (resistente a outliers)\n",
        "        Formula: X_scaled = (X - median) / IQR\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"ROBUST SCALING\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        for col in colunas:\n",
        "            if col in self.df.columns:\n",
        "                scaler = RobustScaler()\n",
        "                valores_originais = self.df[col].values.reshape(-1, 1)\n",
        "                valores_escalados = scaler.fit_transform(valores_originais)\n",
        "\n",
        "                self.df[f'{col}_robust'] = valores_escalados\n",
        "                self.scalers[f'{col}_robust'] = scaler\n",
        "\n",
        "                print(f\"✓ {col} escalado com Robust Scaling\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def aplicar_log_transform(self, colunas):\n",
        "        \"\"\"\n",
        "        Transformação logarítmica: útil para distribuições assimétricas\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"TRANSFORMAÇÃO LOGARÍTMICA\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        for col in colunas:\n",
        "            if col in self.df.columns:\n",
        "                # Log(x + 1) para evitar log(0)\n",
        "                self.df[f'{col}_log'] = np.log1p(self.df[col])\n",
        "\n",
        "                print(f\"✓ {col}:\")\n",
        "                print(f\"  Assimetria original: {self.df[col].skew():.2f}\")\n",
        "                print(f\"  Assimetria log: {self.df[f'{col}_log'].skew():.2f}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    # ==================== ENCODING DE VARIÁVEIS CATEGÓRICAS ====================\n",
        "\n",
        "    def aplicar_label_encoding(self, colunas):\n",
        "        \"\"\"\n",
        "        Label Encoding: converte categorias em números inteiros\n",
        "        Adequado para variáveis ordinais (com ordem)\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"LABEL ENCODING\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        for col in colunas:\n",
        "            if col in self.df.columns:\n",
        "                le = LabelEncoder()\n",
        "                self.df[f'{col}_label'] = le.fit_transform(self.df[col].astype(str))\n",
        "                self.encoders[f'{col}_label'] = le\n",
        "\n",
        "                print(f\"✓ {col}:\")\n",
        "                print(f\"  Categorias únicas: {self.df[col].nunique()}\")\n",
        "                print(f\"  Mapeamento: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def aplicar_onehot_encoding(self, colunas, drop_first=False):\n",
        "        \"\"\"\n",
        "        One-Hot Encoding: cria coluna binária para cada categoria\n",
        "        Adequado para variáveis nominais (sem ordem)\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"ONE-HOT ENCODING\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        for col in colunas:\n",
        "            if col in self.df.columns:\n",
        "                # Criar dummies\n",
        "                dummies = pd.get_dummies(\n",
        "                    self.df[col],\n",
        "                    prefix=col,\n",
        "                    drop_first=drop_first\n",
        "                )\n",
        "\n",
        "                # Adicionar ao dataframe\n",
        "                self.df = pd.concat([self.df, dummies], axis=1)\n",
        "\n",
        "                print(f\"✓ {col}:\")\n",
        "                print(f\"  Categorias: {self.df[col].nunique()}\")\n",
        "                print(f\"  Colunas criadas: {len(dummies.columns)}\")\n",
        "                print(f\"  Nomes: {list(dummies.columns)}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def aplicar_frequency_encoding(self, colunas):\n",
        "        \"\"\"\n",
        "        Frequency Encoding: substitui categoria pela sua frequência\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"FREQUENCY ENCODING\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        for col in colunas:\n",
        "            if col in self.df.columns:\n",
        "                freq = self.df[col].value_counts(normalize=True)\n",
        "                self.df[f'{col}_freq'] = self.df[col].map(freq)\n",
        "\n",
        "                print(f\"✓ {col}: frequências mapeadas\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def aplicar_target_encoding(self, coluna_categorica, coluna_target):\n",
        "        \"\"\"\n",
        "        Target Encoding: substitui categoria pela média do target\n",
        "        Útil para machine learning\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"TARGET ENCODING\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        if coluna_categorica in self.df.columns and coluna_target in self.df.columns:\n",
        "            target_means = self.df.groupby(coluna_categorica)[coluna_target].mean()\n",
        "            self.df[f'{coluna_categorica}_target'] = self.df[coluna_categorica].map(target_means)\n",
        "\n",
        "            print(f\"✓ {coluna_categorica} codificado com médias de {coluna_target}\")\n",
        "            print(f\"  Mapeamento:\")\n",
        "            for cat, mean_val in target_means.items():\n",
        "                print(f\"    {cat}: {mean_val:.2f}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    # ==================== DISCRETIZAÇÃO (BINNING) ====================\n",
        "\n",
        "    def aplicar_binning_igual(self, coluna, n_bins, labels=None):\n",
        "        \"\"\"\n",
        "        Binning com intervalos de largura igual\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"BINNING - INTERVALOS IGUAIS\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        if coluna in self.df.columns:\n",
        "            self.df[f'{coluna}_bin'] = pd.cut(\n",
        "                self.df[coluna],\n",
        "                bins=n_bins,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            print(f\"✓ {coluna} dividido em {n_bins} bins\")\n",
        "            print(f\"  Distribuição:\")\n",
        "            print(self.df[f'{coluna}_bin'].value_counts().sort_index())\n",
        "\n",
        "        return self\n",
        "\n",
        "    def aplicar_binning_quantil(self, coluna, n_bins, labels=None):\n",
        "        \"\"\"\n",
        "        Binning com bins de tamanho igual (mesmo número de observações)\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"BINNING - QUANTIS\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        if coluna in self.df.columns:\n",
        "            self.df[f'{coluna}_qbin'] = pd.qcut(\n",
        "                self.df[coluna],\n",
        "                q=n_bins,\n",
        "                labels=labels,\n",
        "                duplicates='drop'\n",
        "            )\n",
        "\n",
        "            print(f\"✓ {coluna} dividido em {n_bins} quantis\")\n",
        "            print(f\"  Distribuição:\")\n",
        "            print(self.df[f'{coluna}_qbin'].value_counts().sort_index())\n",
        "\n",
        "        return self\n",
        "\n",
        "    def aplicar_binning_customizado(self, coluna, bins, labels):\n",
        "        \"\"\"\n",
        "        Binning com intervalos customizados\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"BINNING - CUSTOMIZADO\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        if coluna in self.df.columns:\n",
        "            self.df[f'{coluna}_custom'] = pd.cut(\n",
        "                self.df[coluna],\n",
        "                bins=bins,\n",
        "                labels=labels,\n",
        "                include_lowest=True\n",
        "            )\n",
        "\n",
        "            print(f\"✓ {coluna} dividido com bins customizados\")\n",
        "            print(f\"  Bins: {bins}\")\n",
        "            print(f\"  Labels: {labels}\")\n",
        "            print(f\"  Distribuição:\")\n",
        "            print(self.df[f'{coluna}_custom'].value_counts().sort_index())\n",
        "\n",
        "        return self\n",
        "\n",
        "    # ==================== FEATURE ENGINEERING ====================\n",
        "\n",
        "    def criar_features_temporais(self, coluna_data):\n",
        "        \"\"\"\n",
        "        Extrai features de colunas de data/hora\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"FEATURE ENGINEERING - TEMPORAL\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        if coluna_data in self.df.columns:\n",
        "            # Garantir que é datetime\n",
        "            self.df[coluna_data] = pd.to_datetime(self.df[coluna_data])\n",
        "\n",
        "            # Extrair componentes\n",
        "            self.df[f'{coluna_data}_ano'] = self.df[coluna_data].dt.year\n",
        "            self.df[f'{coluna_data}_mes'] = self.df[coluna_data].dt.month\n",
        "            self.df[f'{coluna_data}_dia'] = self.df[coluna_data].dt.day\n",
        "            self.df[f'{coluna_data}_dia_semana'] = self.df[coluna_data].dt.dayofweek\n",
        "            self.df[f'{coluna_data}_dia_ano'] = self.df[coluna_data].dt.dayofyear\n",
        "            self.df[f'{coluna_data}_semana'] = self.df[coluna_data].dt.isocalendar().week\n",
        "            self.df[f'{coluna_data}_trimestre'] = self.df[coluna_data].dt.quarter\n",
        "\n",
        "            # Features booleanas\n",
        "            self.df[f'{coluna_data}_fim_semana'] = self.df[coluna_data].dt.dayofweek.isin([5, 6])\n",
        "\n",
        "            print(f\"✓ Features temporais criadas para {coluna_data}\")\n",
        "            print(f\"  Colunas criadas: ano, mes, dia, dia_semana, dia_ano, semana, trimestre, fim_semana\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def criar_features_agregadas(self, grupo_cols, agg_col, agg_funcs):\n",
        "        \"\"\"\n",
        "        Cria features agregadas baseadas em agrupamentos\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"FEATURE ENGINEERING - AGREGAÇÕES\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        for func in agg_funcs:\n",
        "            nome_feature = f'{agg_col}_{func}_by_{\"_\".join(grupo_cols)}'\n",
        "\n",
        "            if func == 'mean':\n",
        "                agg_values = self.df.groupby(grupo_cols)[agg_col].transform('mean')\n",
        "            elif func == 'sum':\n",
        "                agg_values = self.df.groupby(grupo_cols)[agg_col].transform('sum')\n",
        "            elif func == 'count':\n",
        "                agg_values = self.df.groupby(grupo_cols)[agg_col].transform('count')\n",
        "            elif func == 'std':\n",
        "                agg_values = self.df.groupby(grupo_cols)[agg_col].transform('std')\n",
        "            elif func == 'min':\n",
        "                agg_values = self.df.groupby(grupo_cols)[agg_col].transform('min')\n",
        "            elif func == 'max':\n",
        "                agg_values = self.df.groupby(grupo_cols)[agg_col].transform('max')\n",
        "\n",
        "            self.df[nome_feature] = agg_values\n",
        "            print(f\"✓ {nome_feature}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def criar_features_interacao(self, col1, col2, operacao='multiplicar'):\n",
        "        \"\"\"\n",
        "        Cria features de interação entre duas colunas\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"FEATURE ENGINEERING - INTERAÇÕES\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        if col1 in self.df.columns and col2 in self.df.columns:\n",
        "            if operacao == 'multiplicar':\n",
        "                self.df[f'{col1}_x_{col2}'] = self.df[col1] * self.df[col2]\n",
        "                print(f\"✓ {col1} × {col2}\")\n",
        "            elif operacao == 'dividir':\n",
        "                self.df[f'{col1}_div_{col2}'] = self.df[col1] / (self.df[col2] + 1e-10)\n",
        "                print(f\"✓ {col1} ÷ {col2}\")\n",
        "            elif operacao == 'somar':\n",
        "                self.df[f'{col1}_mais_{col2}'] = self.df[col1] + self.df[col2]\n",
        "                print(f\"✓ {col1} + {col2}\")\n",
        "            elif operacao == 'subtrair':\n",
        "                self.df[f'{col1}_menos_{col2}'] = self.df[col1] - self.df[col2]\n",
        "                print(f\"✓ {col1} - {col2}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def criar_features_polinomiais(self, coluna, grau=2):\n",
        "        \"\"\"\n",
        "        Cria features polinomiais (quadrado, cubo, etc.)\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"FEATURE ENGINEERING - POLINOMIAIS\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        if coluna in self.df.columns:\n",
        "            for g in range(2, grau + 1):\n",
        "                self.df[f'{coluna}_pow{g}'] = self.df[coluna] ** g\n",
        "                print(f\"✓ {coluna}^{g}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    # ==================== REDUÇÃO DE DIMENSIONALIDADE ====================\n",
        "\n",
        "    def aplicar_pca(self, colunas, n_components=2):\n",
        "        \"\"\"\n",
        "        Aplica PCA (Principal Component Analysis)\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"PCA - REDUÇÃO DE DIMENSIONALIDADE\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        # Verificar se todas as colunas existem\n",
        "        colunas_validas = [col for col in colunas if col in self.df.columns]\n",
        "\n",
        "        if len(colunas_validas) > 0:\n",
        "            # Dados para PCA (sem missing)\n",
        "            X = self.df[colunas_validas].dropna()\n",
        "\n",
        "            # Aplicar PCA\n",
        "            pca = PCA(n_components=n_components)\n",
        "            componentes = pca.fit_transform(X)\n",
        "\n",
        "            # Adicionar componentes ao dataframe\n",
        "            for i in range(n_components):\n",
        "                self.df.loc[X.index, f'PC{i+1}'] = componentes[:, i]\n",
        "\n",
        "            # Informações\n",
        "            print(f\"✓ PCA aplicado em {len(colunas_validas)} colunas\")\n",
        "            print(f\"  Componentes: {n_components}\")\n",
        "            print(f\"  Variância explicada:\")\n",
        "            for i, var in enumerate(pca.explained_variance_ratio_):\n",
        "                print(f\"    PC{i+1}: {var*100:.2f}%\")\n",
        "            print(f\"  Variância total explicada: {sum(pca.explained_variance_ratio_)*100:.2f}%\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    # ==================== AGREGAÇÃO E PIVOTEAMENTO ====================\n",
        "\n",
        "    def agregar_por_grupo(self, grupo_cols, agg_dict):\n",
        "        \"\"\"\n",
        "        Agrega dados por grupo\n",
        "        agg_dict: dicionário com {coluna: [funções de agregação]}\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"AGREGAÇÃO POR GRUPO\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        df_agg = self.df.groupby(grupo_cols).agg(agg_dict).reset_index()\n",
        "\n",
        "        # Achatar nomes de colunas multi-nível\n",
        "        df_agg.columns = ['_'.join(col).strip('_') if col[1] else col[0]\n",
        "                          for col in df_agg.columns.values]\n",
        "\n",
        "        print(f\"✓ Dados agregados por {grupo_cols}\")\n",
        "        print(f\"  Shape resultante: {df_agg.shape}\")\n",
        "        print(f\"  Colunas: {list(df_agg.columns)}\")\n",
        "\n",
        "        return df_agg\n",
        "\n",
        "    def criar_pivot_table(self, index, columns, values, aggfunc='mean'):\n",
        "        \"\"\"\n",
        "        Cria tabela pivotada\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"PIVOT TABLE\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        pivot = pd.pivot_table(\n",
        "            self.df,\n",
        "            index=index,\n",
        "            columns=columns,\n",
        "            values=values,\n",
        "            aggfunc=aggfunc,\n",
        "            fill_value=0\n",
        "        )\n",
        "\n",
        "        print(f\"✓ Pivot table criada\")\n",
        "        print(f\"  Index: {index}\")\n",
        "        print(f\"  Columns: {columns}\")\n",
        "        print(f\"  Values: {values}\")\n",
        "        print(f\"  Shape: {pivot.shape}\")\n",
        "\n",
        "        return pivot\n",
        "\n",
        "    # ==================== UTILIDADES ====================\n",
        "\n",
        "    def obter_df(self):\n",
        "        \"\"\"Retorna dataframe transformado\"\"\"\n",
        "        return self.df\n",
        "\n",
        "    def selecionar_colunas(self, colunas):\n",
        "        \"\"\"Seleciona apenas colunas específicas\"\"\"\n",
        "        self.df = self.df[colunas]\n",
        "        print(f\"✓ Selecionadas {len(colunas)} colunas\")\n",
        "        return self\n",
        "\n",
        "    def remover_colunas(self, colunas):\n",
        "        \"\"\"Remove colunas específicas\"\"\"\n",
        "        self.df = self.df.drop(columns=colunas, errors='ignore')\n",
        "        print(f\"✓ Removidas colunas: {colunas}\")\n",
        "        return self\n",
        "\n",
        "\n",
        "# ==================== EXEMPLO DE USO ====================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Carregar dados limpos\n",
        "    print(\"Carregando dados limpos...\")\n",
        "    df = pd.read_csv('dados_vendas_limpos.csv')\n",
        "\n",
        "    print(f\"\\nDataset: {df.shape[0]} linhas x {df.shape[1]} colunas\")\n",
        "\n",
        "    # Criar instância do transformador\n",
        "    transformador = TransformadorDados(df)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"PIPELINE DE TRANSFORMAÇÃO\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # 1. Features temporais\n",
        "    transformador.criar_features_temporais('data_venda')\n",
        "\n",
        "    # 2. Normalização de variáveis numéricas\n",
        "    transformador.aplicar_standardization(['preco_unitario', 'quantidade'])\n",
        "    transformador.aplicar_min_max_scaling(['preco_unitario'], feature_range=(0, 1))\n",
        "\n",
        "    # 3. Transformação logarítmica para dados assimétricos\n",
        "    transformador.aplicar_log_transform(['valor_total'])\n",
        "\n",
        "    # 4. Encoding de variáveis categóricas\n",
        "    transformador.aplicar_label_encoding(['status'])\n",
        "    transformador.aplicar_onehot_encoding(['produto'], drop_first=True)\n",
        "    transformador.aplicar_frequency_encoding(['cliente'])\n",
        "\n",
        "    # 5. Discretização (Binning)\n",
        "    transformador.aplicar_binning_quantil('preco_unitario', n_bins=4,\n",
        "                                          labels=['Baixo', 'Médio', 'Alto', 'Premium'])\n",
        "\n",
        "    # 6. Feature Engineering\n",
        "    transformador.criar_features_interacao('quantidade', 'preco_unitario', operacao='multiplicar')\n",
        "    transformador.criar_features_polinomiais('quantidade', grau=2)\n",
        "\n",
        "    # 7. Features agregadas\n",
        "    transformador.criar_features_agregadas(\n",
        "        grupo_cols=['produto'],\n",
        "        agg_col='valor_total',\n",
        "        agg_funcs=['mean', 'sum', 'count']\n",
        "    )\n",
        "\n",
        "    # Obter dataframe transformado\n",
        "    df_transformado = transformador.obter_df()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"TRANSFORMAÇÃO CONCLUÍDA\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Shape final: {df_transformado.shape}\")\n",
        "    print(f\"Colunas criadas: {df_transformado.shape[1] - df.shape[1]}\")\n",
        "\n",
        "    # Salvar resultado\n",
        "    df_transformado.to_csv('dados_vendas_transformados.csv', index=False)\n",
        "    print(\"\\n✓ Dados transformados salvos em: dados_vendas_transformados.csv\")\n",
        "\n",
        "    # Exibir amostra\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"AMOSTRA DOS DADOS TRANSFORMADOS (primeiras 3 linhas)\")\n",
        "    print(\"=\"*70)\n",
        "    print(df_transformado.head(3))\n",
        "\n",
        "    # ==================== EXEMPLO DE AGREGAÇÃO ====================\n",
        "\n",
        "    print(\"\\n\\n\" + \"=\"*70)\n",
        "    print(\"EXEMPLO: AGREGAÇÃO DE DADOS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Criar relatório de vendas por produto\n",
        "    relatorio_produto = transformador.agregar_por_grupo(\n",
        "        grupo_cols=['produto'],\n",
        "        agg_dict={\n",
        "            'quantidade': ['sum', 'mean', 'count'],\n",
        "            'valor_total': ['sum', 'mean', 'min', 'max'],\n",
        "            'preco_unitario': ['mean']\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(\"\\nRelatório de Vendas por Produto:\")\n",
        "    print(relatorio_produto)\n",
        "\n",
        "    # ==================== EXEMPLO DE PIVOT TABLE ====================\n",
        "\n",
        "    print(\"\\n\\n\" + \"=\"*70)\n",
        "    print(\"EXEMPLO: PIVOT TABLE\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Criar pivot: vendas por produto e status\n",
        "    if 'status' in df.columns and 'produto' in df.columns:\n",
        "        pivot_vendas = transformador.criar_pivot_table(\n",
        "            index='produto',\n",
        "            columns='status',\n",
        "            values='valor_total',\n",
        "            aggfunc='sum'\n",
        "        )\n",
        "\n",
        "        print(\"\\nPivot: Valor Total por Produto e Status\")\n",
        "        print(pivot_vendas)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"✓ PIPELINE COMPLETO EXECUTADO COM SUCESSO!\")\n",
        "    print(\"=\"*70)"
      ],
      "metadata": {
        "id": "wXlf_y2oC7qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Script 05: Validação e Controle de Qualidade de Dados\n",
        "Sistema completo de regras de validação e métricas de qualidade\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "\n",
        "class ValidadorQualidade:\n",
        "    \"\"\"\n",
        "    Sistema de validação e controle de qualidade de dados\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df):\n",
        "        self.df = df.copy()\n",
        "        self.resultados_validacao = []\n",
        "        self.metricas_qualidade = {}\n",
        "\n",
        "    # ==================== VALIDAÇÕES DE TIPO ====================\n",
        "\n",
        "    def validar_tipo_coluna(self, coluna, tipo_esperado):\n",
        "        \"\"\"\n",
        "        Valida se coluna tem o tipo de dado esperado\n",
        "        tipo_esperado: 'int', 'float', 'string', 'datetime', 'bool'\n",
        "        \"\"\"\n",
        "        if coluna not in self.df.columns:\n",
        "            self._registrar_falha(f\"Coluna '{coluna}' não existe\")\n",
        "            return False\n",
        "\n",
        "        tipo_atual = self.df[coluna].dtype\n",
        "\n",
        "        mapeamento_tipos = {\n",
        "            'int': ['int64', 'int32', 'Int64'],\n",
        "            'float': ['float64', 'float32'],\n",
        "            'string': ['object', 'string'],\n",
        "            'datetime': ['datetime64[ns]'],\n",
        "            'bool': ['bool']\n",
        "        }\n",
        "\n",
        "        tipos_validos = mapeamento_tipos.get(tipo_esperado, [])\n",
        "        validacao_ok = str(tipo_atual) in tipos_validos\n",
        "\n",
        "        if validacao_ok:\n",
        "            self._registrar_sucesso(\n",
        "                f\"✓ '{coluna}': tipo correto ({tipo_atual})\"\n",
        "            )\n",
        "        else:\n",
        "            self._registrar_falha(\n",
        "                f\"✗ '{coluna}': tipo incorreto. Esperado {tipo_esperado}, encontrado {tipo_atual}\"\n",
        "            )\n",
        "\n",
        "        return validacao_ok\n",
        "\n",
        "    # ==================== VALIDAÇÕES DE RANGE ====================\n",
        "\n",
        "    def validar_range_numerico(self, coluna, min_val=None, max_val=None):\n",
        "        \"\"\"\n",
        "        Valida se valores numéricos estão dentro do range esperado\n",
        "        \"\"\"\n",
        "        if coluna not in self.df.columns:\n",
        "            self._registrar_falha(f\"Coluna '{coluna}' não existe\")\n",
        "            return False\n",
        "\n",
        "        valores = self.df[coluna].dropna()\n",
        "\n",
        "        violacoes = 0\n",
        "        if min_val is not None:\n",
        "            violacoes += (valores < min_val).sum()\n",
        "        if max_val is not None:\n",
        "            violacoes += (valores > max_val).sum()\n",
        "\n",
        "        if violacoes == 0:\n",
        "            self._registrar_sucesso(\n",
        "                f\"✓ '{coluna}': todos os valores no range [{min_val}, {max_val}]\"\n",
        "            )\n",
        "            return True\n",
        "        else:\n",
        "            self._registrar_falha(\n",
        "                f\"✗ '{coluna}': {violacoes} valores fora do range [{min_val}, {max_val}]\"\n",
        "            )\n",
        "            return False\n",
        "\n",
        "    def validar_valores_positivos(self, coluna):\n",
        "        \"\"\"Valida se todos os valores são positivos\"\"\"\n",
        "        return self.validar_range_numerico(coluna, min_val=0)\n",
        "\n",
        "    def validar_valores_unicos(self, coluna):\n",
        "        \"\"\"Valida se todos os valores são únicos (chave primária)\"\"\"\n",
        "        if coluna not in self.df.columns:\n",
        "            self._registrar_falha(f\"Coluna '{coluna}' não existe\")\n",
        "            return False\n",
        "\n",
        "        duplicatas = self.df[coluna].duplicated().sum()\n",
        "\n",
        "        if duplicatas == 0:\n",
        "            self._registrar_sucesso(f\"✓ '{coluna}': todos os valores são únicos\")\n",
        "            return True\n",
        "        else:\n",
        "            self._registrar_falha(f\"✗ '{coluna}': {duplicatas} valores duplicados\")\n",
        "            return False\n",
        "\n",
        "    # ==================== VALIDAÇÕES DE COMPLETUDE ====================\n",
        "\n",
        "    def validar_sem_missing(self, colunas):\n",
        "        \"\"\"Valida se colunas não têm valores ausentes\"\"\"\n",
        "        resultado = True\n",
        "\n",
        "        for col in colunas:\n",
        "            if col not in self.df.columns:\n",
        "                self._registrar_falha(f\"Coluna '{col}' não existe\")\n",
        "                resultado = False\n",
        "                continue\n",
        "\n",
        "            missing = self.df[col].isnull().sum()\n",
        "\n",
        "            if missing == 0:\n",
        "                self._registrar_sucesso(f\"✓ '{col}': sem valores ausentes\")\n",
        "            else:\n",
        "                self._registrar_falha(\n",
        "                    f\"✗ '{col}': {missing} valores ausentes ({100*missing/len(self.df):.2f}%)\"\n",
        "                )\n",
        "                resultado = False\n",
        "\n",
        "        return resultado\n",
        "\n",
        "    def validar_completude_minima(self, coluna, percentual_minimo=0.95):\n",
        "        \"\"\"\n",
        "        Valida se coluna tem pelo menos X% de completude\n",
        "        \"\"\"\n",
        "        if coluna not in self.df.columns:\n",
        "            self._registrar_falha(f\"Coluna '{coluna}' não existe\")\n",
        "            return False\n",
        "\n",
        "        completude = 1 - (self.df[coluna].isnull().sum() / len(self.df))\n",
        "\n",
        "        if completude >= percentual_minimo:\n",
        "            self._registrar_sucesso(\n",
        "                f\"✓ '{coluna}': completude {completude*100:.2f}% (>= {percentual_minimo*100}%)\"\n",
        "            )\n",
        "            return True\n",
        "        else:\n",
        "            self._registrar_falha(\n",
        "                f\"✗ '{coluna}': completude {completude*100:.2f}% (< {percentual_minimo*100}%)\"\n",
        "            )\n",
        "            return False\n",
        "\n",
        "    # ==================== VALIDAÇÕES DE FORMATO ====================\n",
        "\n",
        "    def validar_formato_regex(self, coluna, padrao, descricao=\"formato\"):\n",
        "        \"\"\"\n",
        "        Valida se valores seguem um padrão regex\n",
        "        \"\"\"\n",
        "        if coluna not in self.df.columns:\n",
        "            self._registrar_falha(f\"Coluna '{coluna}' não existe\")\n",
        "            return False\n",
        "\n",
        "        valores = self.df[coluna].dropna().astype(str)\n",
        "        matches = valores.str.match(padrao)\n",
        "        invalidos = (~matches).sum()\n",
        "\n",
        "        if invalidos == 0:\n",
        "            self._registrar_sucesso(f\"✓ '{coluna}': todos os valores seguem {descricao}\")\n",
        "            return True\n",
        "        else:\n",
        "            self._registrar_falha(\n",
        "                f\"✗ '{coluna}': {invalidos} valores não seguem {descricao}\"\n",
        "            )\n",
        "            return False\n",
        "\n",
        "    def validar_email(self, coluna):\n",
        "        \"\"\"Valida formato de email\"\"\"\n",
        "        padrao = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
        "        return self.validar_formato_regex(coluna, padrao, \"formato de email\")\n",
        "\n",
        "    def validar_cpf(self, coluna):\n",
        "        \"\"\"Valida formato de CPF (XXX.XXX.XXX-XX)\"\"\"\n",
        "        padrao = r'^\\d{3}\\.\\d{3}\\.\\d{3}-\\d{2}$'\n",
        "        return self.validar_formato_regex(coluna, padrao, \"formato de CPF\")\n",
        "\n",
        "    def validar_telefone_br(self, coluna):\n",
        "        \"\"\"Valida formato de telefone brasileiro\"\"\"\n",
        "        padrao = r'^\\(\\d{2}\\)\\s?\\d{4,5}-?\\d{4}$'\n",
        "        return self.validar_formato_regex(coluna, padrao, \"formato de telefone\")\n",
        "\n",
        "    def validar_cep(self, coluna):\n",
        "        \"\"\"Valida formato de CEP\"\"\"\n",
        "        padrao = r'^\\d{5}-?\\d{3}$'\n",
        "        return self.validar_formato_regex(coluna, padrao, \"formato de CEP\")\n",
        "\n",
        "    # ==================== VALIDAÇÕES DE NEGÓCIO ====================\n",
        "\n",
        "    def validar_valores_permitidos(self, coluna, valores_validos):\n",
        "        \"\"\"\n",
        "        Valida se coluna contém apenas valores da lista permitida\n",
        "        \"\"\"\n",
        "        if coluna not in self.df.columns:\n",
        "            self._registrar_falha(f\"Coluna '{coluna}' não existe\")\n",
        "            return False\n",
        "\n",
        "        valores_invalidos = ~self.df[coluna].isin(valores_validos + [np.nan, None])\n",
        "        count_invalidos = valores_invalidos.sum()\n",
        "\n",
        "        if count_invalidos == 0:\n",
        "            self._registrar_sucesso(\n",
        "                f\"✓ '{coluna}': todos os valores são válidos\"\n",
        "            )\n",
        "            return True\n",
        "        else:\n",
        "            invalidos_unicos = self.df[valores_invalidos][coluna].unique()\n",
        "            self._registrar_falha(\n",
        "                f\"✗ '{coluna}': {count_invalidos} valores inválidos. \"\n",
        "                f\"Encontrados: {list(invalidos_unicos[:5])}\"\n",
        "            )\n",
        "            return False\n",
        "\n",
        "    def validar_relacao_colunas(self, col1, col2, operacao, descricao=\"\"):\n",
        "        \"\"\"\n",
        "        Valida relação entre duas colunas\n",
        "        operacao: '>', '<', '>=', '<=', '==', '!='\n",
        "        \"\"\"\n",
        "        if col1 not in self.df.columns or col2 not in self.df.columns:\n",
        "            self._registrar_falha(f\"Uma ou mais colunas não existem\")\n",
        "            return False\n",
        "\n",
        "        if operacao == '>':\n",
        "            violacoes = (self.df[col1] <= self.df[col2]).sum()\n",
        "        elif operacao == '<':\n",
        "            violacoes = (self.df[col1] >= self.df[col2]).sum()\n",
        "        elif operacao == '>=':\n",
        "            violacoes = (self.df[col1] < self.df[col2]).sum()\n",
        "        elif operacao == '<=':\n",
        "            violacoes = (self.df[col1] > self.df[col2]).sum()\n",
        "        elif operacao == '==':\n",
        "            violacoes = (self.df[col1] != self.df[col2]).sum()\n",
        "        elif operacao == '!=':\n",
        "            violacoes = (self.df[col1] == self.df[col2]).sum()\n",
        "        else:\n",
        "            self._registrar_falha(f\"Operação '{operacao}' inválida\")\n",
        "            return False\n",
        "\n",
        "        if violacoes == 0:\n",
        "            self._registrar_sucesso(\n",
        "                f\"✓ Relação '{col1} {operacao} {col2}': válida {descricao}\"\n",
        "            )\n",
        "            return True\n",
        "        else:\n",
        "            self._registrar_falha(\n",
        "                f\"✗ Relação '{col1} {operacao} {col2}': {violacoes} violações {descricao}\"\n",
        "            )\n",
        "            return False\n",
        "\n",
        "    def validar_soma_coluna(self, colunas, coluna_total, tolerancia=0.01):\n",
        "        \"\"\"\n",
        "        Valida se soma de colunas resulta em coluna total\n",
        "        \"\"\"\n",
        "        soma_calculada = self.df[colunas].sum(axis=1)\n",
        "        diferenca = abs(soma_calculada - self.df[coluna_total])\n",
        "        violacoes = (diferenca > tolerancia).sum()\n",
        "\n",
        "        if violacoes == 0:\n",
        "            self._registrar_sucesso(\n",
        "                f\"✓ Soma de {colunas} = {coluna_total} (tolerância: {tolerancia})\"\n",
        "            )\n",
        "            return True\n",
        "        else:\n",
        "            self._registrar_falha(\n",
        "                f\"✗ {violacoes} inconsistências na soma de {colunas} = {coluna_total}\"\n",
        "            )\n",
        "            return False\n",
        "\n",
        "    # ==================== MÉTRICAS DE QUALIDADE ====================\n",
        "\n",
        "    def calcular_metricas_qualidade(self):\n",
        "        \"\"\"\n",
        "        Calcula métricas abrangentes de qualidade\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"MÉTRICAS DE QUALIDADE DE DADOS\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # 1. Completude\n",
        "        total_cells = self.df.shape[0] * self.df.shape[1]\n",
        "        missing_cells = self.df.isnull().sum().sum()\n",
        "        completude = 100 * (1 - missing_cells / total_cells)\n",
        "\n",
        "        self.metricas_qualidade['completude_geral'] = completude\n",
        "        print(f\"\\n1. COMPLETUDE GERAL: {completude:.2f}%\")\n",
        "\n",
        "        # Completude por coluna\n",
        "        completude_cols = {}\n",
        "        for col in self.df.columns:\n",
        "            comp = 100 * (1 - self.df[col].isnull().sum() / len(self.df))\n",
        "            completude_cols[col] = comp\n",
        "\n",
        "        self.metricas_qualidade['completude_por_coluna'] = completude_cols\n",
        "\n",
        "        # 2. Unicidade\n",
        "        duplicatas = self.df.duplicated().sum()\n",
        "        unicidade = 100 * (1 - duplicatas / len(self.df))\n",
        "\n",
        "        self.metricas_qualidade['unicidade'] = unicidade\n",
        "        print(f\"2. UNICIDADE: {unicidade:.2f}%\")\n",
        "        print(f\"   Registros duplicados: {duplicatas}\")\n",
        "\n",
        "        # 3. Consistência de tipos\n",
        "        tipos_inconsistentes = 0\n",
        "        for col in self.df.columns:\n",
        "            if self.df[col].dtype == 'object':\n",
        "                # Verificar se valores podem ser numéricos\n",
        "                try:\n",
        "                    pd.to_numeric(self.df[col].dropna(), errors='raise')\n",
        "                    tipos_inconsistentes += 1\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        consistencia_tipos = 100 * (1 - tipos_inconsistentes / len(self.df.columns))\n",
        "        self.metricas_qualidade['consistencia_tipos'] = consistencia_tipos\n",
        "        print(f\"3. CONSISTÊNCIA DE TIPOS: {consistencia_tipos:.2f}%\")\n",
        "\n",
        "        # 4. Score de Qualidade Geral\n",
        "        score_qualidade = (completude + unicidade + consistencia_tipos) / 3\n",
        "        self.metricas_qualidade['score_geral'] = score_qualidade\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"SCORE GERAL DE QUALIDADE: {score_qualidade:.2f}%\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        # Classificação\n",
        "        if score_qualidade >= 90:\n",
        "            classificacao = \"EXCELENTE\"\n",
        "        elif score_qualidade >= 75:\n",
        "            classificacao = \"BOM\"\n",
        "        elif score_qualidade >= 60:\n",
        "            classificacao = \"REGULAR\"\n",
        "        else:\n",
        "            classificacao = \"NECESSITA MELHORIAS\"\n",
        "\n",
        "        print(f\"Classificação: {classificacao}\")\n",
        "\n",
        "        return self.metricas_qualidade\n",
        "\n",
        "    # ==================== RELATÓRIOS ====================\n",
        "\n",
        "    def _registrar_falha(self, mensagem):\n",
        "        \"\"\"Registra validação que falhou\"\"\"\n",
        "        self.resultados_validacao.append({\n",
        "            'status': 'FALHA',\n",
        "            'mensagem': mensagem,\n",
        "            'timestamp': datetime.now()\n",
        "        })\n",
        "\n",
        "    def gerar_relatorio_validacao(self):\n",
        "        \"\"\"\n",
        "        Gera relatório completo de validação\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"RELATÓRIO DE VALIDAÇÃO\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        total = len(self.resultados_validacao)\n",
        "        sucessos = sum(1 for r in self.resultados_validacao if r['status'] == 'SUCESSO')\n",
        "        falhas = sum(1 for r in self.resultados_validacao if r['status'] == 'FALHA')\n",
        "\n",
        "        print(f\"\\nTotal de validações: {total}\")\n",
        "        print(f\"Sucessos: {sucessos} ({100*sucessos/total if total > 0 else 0:.1f}%)\")\n",
        "        print(f\"Falhas: {falhas} ({100*falhas/total if total > 0 else 0:.1f}%)\")\n",
        "\n",
        "        print(\"\\n\" + \"-\"*70)\n",
        "        print(\"DETALHES DAS VALIDAÇÕES\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        for resultado in self.resultados_validacao:\n",
        "            print(resultado['mensagem'])\n",
        "\n",
        "        return sucessos, falhas\n",
        "\n",
        "    def exportar_relatorio(self, nome_arquivo='relatorio_validacao.txt'):\n",
        "        \"\"\"\n",
        "        Exporta relatório de validação para arquivo\n",
        "        \"\"\"\n",
        "        with open(nome_arquivo, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"=\"*70 + \"\\n\")\n",
        "            f.write(\"RELATÓRIO DE VALIDAÇÃO DE DADOS\\n\")\n",
        "            f.write(f\"Gerado em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "            f.write(\"=\"*70 + \"\\n\\n\")\n",
        "\n",
        "            for resultado in self.resultados_validacao:\n",
        "                f.write(f\"[{resultado['status']}] {resultado['mensagem']}\\n\")\n",
        "\n",
        "            f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "            f.write(\"MÉTRICAS DE QUALIDADE\\n\")\n",
        "            f.write(\"=\"*70 + \"\\n\")\n",
        "\n",
        "            for metrica, valor in self.metricas_qualidade.items():\n",
        "                if isinstance(valor, dict):\n",
        "                    f.write(f\"\\n{metrica}:\\n\")\n",
        "                    for k, v in valor.items():\n",
        "                        f.write(f\"  {k}: {v:.2f}%\\n\")\n",
        "                else:\n",
        "                    f.write(f\"{metrica}: {valor:.2f}%\\n\")\n",
        "\n",
        "        print(f\"\\n✓ Relatório exportado: {nome_arquivo}\")\n",
        "\n",
        "\n",
        "# ==================== FUNÇÕES AUXILIARES ====================\n",
        "\n",
        "def criar_suite_validacao_vendas(df):\n",
        "    \"\"\"\n",
        "    Cria suite de validação específica para dados de vendas\n",
        "    \"\"\"\n",
        "    validador = ValidadorQualidade(df)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"SUITE DE VALIDAÇÃO: DADOS DE VENDAS\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # 1. Validações de tipo\n",
        "    print(\"1. VALIDAÇÃO DE TIPOS\")\n",
        "    print(\"-\"*70)\n",
        "    validador.validar_tipo_coluna('id_venda', 'int')\n",
        "    validador.validar_tipo_coluna('data_venda', 'datetime')\n",
        "    validador.validar_tipo_coluna('quantidade', 'int')\n",
        "    validador.validar_tipo_coluna('preco_unitario', 'float')\n",
        "\n",
        "    # 2. Validações de completude\n",
        "    print(\"\\n2. VALIDAÇÃO DE COMPLETUDE\")\n",
        "    print(\"-\"*70)\n",
        "    validador.validar_sem_missing(['id_venda', 'data_venda', 'produto'])\n",
        "    validador.validar_completude_minima('preco_unitario', 0.95)\n",
        "    validador.validar_completude_minima('quantidade', 0.95)\n",
        "\n",
        "    # 3. Validações de unicidade\n",
        "    print(\"\\n3. VALIDAÇÃO DE UNICIDADE\")\n",
        "    print(\"-\"*70)\n",
        "    validador.validar_valores_unicos('id_venda')\n",
        "\n",
        "    # 4. Validações de range\n",
        "    print(\"\\n4. VALIDAÇÃO DE RANGES\")\n",
        "    print(\"-\"*70)\n",
        "    validador.validar_valores_positivos('quantidade')\n",
        "    validador.validar_valores_positivos('preco_unitario')\n",
        "    validador.validar_valores_positivos('valor_total')\n",
        "    validador.validar_range_numerico('quantidade', min_val=1, max_val=100)\n",
        "\n",
        "    # 5. Validações de negócio\n",
        "    print(\"\\n5. VALIDAÇÃO DE REGRAS DE NEGÓCIO\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    if 'status' in df.columns:\n",
        "        status_validos = ['Concluído', 'Pendente', 'Cancelado', 'Em Processamento']\n",
        "        validador.validar_valores_permitidos('status', status_validos)\n",
        "\n",
        "    # Validar se valor_total = quantidade * preco_unitario\n",
        "    if all(col in df.columns for col in ['quantidade', 'preco_unitario', 'valor_total']):\n",
        "        df_temp = df.dropna(subset=['quantidade', 'preco_unitario', 'valor_total'])\n",
        "        valor_calculado = df_temp['quantidade'] * df_temp['preco_unitario']\n",
        "        diferenca = abs(valor_calculado - df_temp['valor_total'])\n",
        "        violacoes = (diferenca > 0.01).sum()\n",
        "\n",
        "        if violacoes == 0:\n",
        "            validador._registrar_sucesso(\n",
        "                \"✓ valor_total = quantidade × preco_unitario\"\n",
        "            )\n",
        "        else:\n",
        "            validador._registrar_falha(\n",
        "                f\"✗ {violacoes} inconsistências: valor_total ≠ quantidade × preco_unitario\"\n",
        "            )\n",
        "\n",
        "    return validador\n",
        "\n",
        "\n",
        "def criar_suite_validacao_sensores(df):\n",
        "    \"\"\"\n",
        "    Cria suite de validação para dados de sensores IoT\n",
        "    \"\"\"\n",
        "    validador = ValidadorQualidade(df)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"SUITE DE VALIDAÇÃO: DADOS DE SENSORES IOT\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # 1. Validações básicas\n",
        "    print(\"1. VALIDAÇÃO BÁSICA\")\n",
        "    print(\"-\"*70)\n",
        "    validador.validar_sem_missing(['timestamp', 'sensor_id', 'unidade'])\n",
        "    validador.validar_tipo_coluna('timestamp', 'datetime')\n",
        "\n",
        "    # 2. Validações de range por tipo de sensor\n",
        "    print(\"\\n2. VALIDAÇÃO DE RANGES POR SENSOR\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    if 'sensor_id' in df.columns and 'valor' in df.columns:\n",
        "        # Temperatura: -10 a 50°C\n",
        "        df_temp = df[df['sensor_id'].str.contains('TEMP', na=False)]\n",
        "        if len(df_temp) > 0:\n",
        "            fora_range = ((df_temp['valor'] < -10) | (df_temp['valor'] > 50)).sum()\n",
        "            if fora_range == 0:\n",
        "                validador._registrar_sucesso(\n",
        "                    \"✓ Sensores TEMP: valores no range [-10°C, 50°C]\"\n",
        "                )\n",
        "            else:\n",
        "                validador._registrar_falha(\n",
        "                    f\"✗ Sensores TEMP: {fora_range} valores fora do range [-10°C, 50°C]\"\n",
        "                )\n",
        "\n",
        "        # Umidade: 0 a 100%\n",
        "        df_humid = df[df['sensor_id'].str.contains('HUMID', na=False)]\n",
        "        if len(df_humid) > 0:\n",
        "            fora_range = ((df_humid['valor'] < 0) | (df_humid['valor'] > 100)).sum()\n",
        "            if fora_range == 0:\n",
        "                validador._registrar_sucesso(\n",
        "                    \"✓ Sensores HUMID: valores no range [0%, 100%]\"\n",
        "                )\n",
        "            else:\n",
        "                validador._registrar_falha(\n",
        "                    f\"✗ Sensores HUMID: {fora_range} valores fora do range [0%, 100%]\"\n",
        "                )\n",
        "\n",
        "        # Pressão: 900 a 1100 hPa\n",
        "        df_press = df[df['sensor_id'].str.contains('PRESS', na=False)]\n",
        "        if len(df_press) > 0:\n",
        "            fora_range = ((df_press['valor'] < 900) | (df_press['valor'] > 1100)).sum()\n",
        "            if fora_range == 0:\n",
        "                validador._registrar_sucesso(\n",
        "                    \"✓ Sensores PRESS: valores no range [900 hPa, 1100 hPa]\"\n",
        "                )\n",
        "            else:\n",
        "                validador._registrar_falha(\n",
        "                    f\"✗ Sensores PRESS: {fora_range} valores fora do range [900 hPa, 1100 hPa]\"\n",
        "                )\n",
        "\n",
        "    # 3. Validação de duplicatas temporais\n",
        "    print(\"\\n3. VALIDAÇÃO DE TIMESTAMPS\")\n",
        "    print(\"-\"*70)\n",
        "    if 'timestamp' in df.columns and 'sensor_id' in df.columns:\n",
        "        duplicatas = df.duplicated(subset=['timestamp', 'sensor_id']).sum()\n",
        "        if duplicatas == 0:\n",
        "            validador._registrar_sucesso(\n",
        "                \"✓ Sem duplicatas de timestamp por sensor\"\n",
        "            )\n",
        "        else:\n",
        "            validador._registrar_falha(\n",
        "                f\"✗ {duplicatas} duplicatas de timestamp por sensor\"\n",
        "            )\n",
        "\n",
        "    return validador\n",
        "\n",
        "\n",
        "# ==================== EXEMPLO DE USO ====================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Carregar dados\n",
        "    print(\"Carregando dados limpos...\")\n",
        "    df_vendas = pd.read_csv('dados_vendas_limpos.csv')\n",
        "\n",
        "    # Converter data_venda para datetime se necessário\n",
        "    if 'data_venda' in df_vendas.columns:\n",
        "        df_vendas['data_venda'] = pd.to_datetime(df_vendas['data_venda'])\n",
        "\n",
        "    # Executar suite de validação\n",
        "    validador = criar_suite_validacao_vendas(df_vendas)\n",
        "\n",
        "    # Calcular métricas de qualidade\n",
        "    metricas = validador.calcular_metricas_qualidade()\n",
        "\n",
        "    # Gerar relatório\n",
        "    sucessos, falhas = validador.gerar_relatorio_validacao()\n",
        "\n",
        "    # Exportar relatório\n",
        "    validador.exportar_relatorio('relatorio_validacao_vendas.txt')\n",
        "\n",
        "    # ==================== VALIDAÇÃO DE SENSORES ====================\n",
        "\n",
        "    print(\"\\n\\n\" + \"=\"*70)\n",
        "    print(\"VALIDAÇÃO DE DADOS DE SENSORES\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    try:\n",
        "        df_sensores = pd.read_csv('dados_sensores_raw.csv')\n",
        "        df_sensores['timestamp'] = pd.to_datetime(df_sensores['timestamp'])\n",
        "\n",
        "        validador_sensores = criar_suite_validacao_sensores(df_sensores)\n",
        "        validador_sensores.calcular_metricas_qualidade()\n",
        "        validador_sensores.gerar_relatorio_validacao()\n",
        "        validador_sensores.exportar_relatorio('relatorio_validacao_sensores.txt')\n",
        "    except FileNotFoundError:\n",
        "        print(\"Arquivo de sensores não encontrado, pulando validação...\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"✓ VALIDAÇÃO COMPLETA\")\n",
        "    print(\"=\"*70)"
      ],
      "metadata": {
        "id": "jSAHFIlADCB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Script 06: Pipeline Completo de Pré-processamento\n",
        "Integra todas as etapas: exploração, limpeza, transformação e validação\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "class PipelinePreProcessamento:\n",
        "    \"\"\"\n",
        "    Pipeline completo e configurável de pré-processamento de dados\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nome_pipeline=\"Pipeline\"):\n",
        "        self.nome_pipeline = nome_pipeline\n",
        "        self.df_original = None\n",
        "        self.df_processado = None\n",
        "        self.etapas_executadas = []\n",
        "        self.metricas = {}\n",
        "        self.tempo_inicio = None\n",
        "        self.tempo_fim = None\n",
        "\n",
        "    def carregar_dados(self, caminho_arquivo, **kwargs):\n",
        "        \"\"\"\n",
        "        Carrega dados de arquivo CSV\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"PIPELINE: {self.nome_pipeline}\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "        print(f\"📁 Carregando dados de: {caminho_arquivo}\")\n",
        "\n",
        "        self.tempo_inicio = datetime.now()\n",
        "\n",
        "        try:\n",
        "            self.df_original = pd.read_csv(caminho_arquivo, **kwargs)\n",
        "            self.df_processado = self.df_original.copy()\n",
        "\n",
        "            print(f\"✓ Dados carregados com sucesso!\")\n",
        "            print(f\"  Shape: {self.df_original.shape}\")\n",
        "            print(f\"  Memória: {self.df_original.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "            self._registrar_etapa(\"Carregamento\", {\n",
        "                'linhas': len(self.df_original),\n",
        "                'colunas': len(self.df_original.columns),\n",
        "                'memoria_mb': self.df_original.memory_usage(deep=True).sum() / 1024**2\n",
        "            })\n",
        "\n",
        "            return self\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Erro ao carregar dados: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def explorar_dados(self):\n",
        "        \"\"\"\n",
        "        Realiza exploração inicial dos dados\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"ETAPA 1: EXPLORAÇÃO DE DADOS\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        # Informações básicas\n",
        "        print(\"📊 Informações Básicas:\")\n",
        "        print(f\"  Linhas: {len(self.df_processado):,}\")\n",
        "        print(f\"  Colunas: {len(self.df_processado.columns)}\")\n",
        "        print(f\"  Colunas: {list(self.df_processado.columns)}\")\n",
        "\n",
        "        # Tipos de dados\n",
        "        print(f\"\\n📋 Tipos de Dados:\")\n",
        "        for dtype, count in self.df_processado.dtypes.value_counts().items():\n",
        "            print(f\"  {dtype}: {count} colunas\")\n",
        "\n",
        "        # Valores ausentes\n",
        "        missing_total = self.df_processado.isnull().sum().sum()\n",
        "        missing_percent = 100 * missing_total / (len(self.df_processado) * len(self.df_processado.columns))\n",
        "\n",
        "        print(f\"\\n❓ Valores Ausentes:\")\n",
        "        print(f\"  Total: {missing_total:,} ({missing_percent:.2f}%)\")\n",
        "\n",
        "        if missing_total > 0:\n",
        "            print(f\"  Por coluna:\")\n",
        "            for col in self.df_processado.columns:\n",
        "                missing = self.df_processado[col].isnull().sum()\n",
        "                if missing > 0:\n",
        "                    print(f\"    {col}: {missing} ({100*missing/len(self.df_processado):.2f}%)\")\n",
        "\n",
        "        # Duplicatas\n",
        "        duplicatas = self.df_processado.duplicated().sum()\n",
        "        print(f\"\\n🔄 Duplicatas: {duplicatas} ({100*duplicatas/len(self.df_processado):.2f}%)\")\n",
        "\n",
        "        self._registrar_etapa(\"Exploração\", {\n",
        "            'missing_total': int(missing_total),\n",
        "            'missing_percent': float(missing_percent),\n",
        "            'duplicatas': int(duplicatas)\n",
        "        })\n",
        "\n",
        "        return self\n",
        "\n",
        "    def limpar_dados(self, config):\n",
        "        \"\"\"\n",
        "        Aplica limpeza de dados baseada em configuração\n",
        "\n",
        "        config = {\n",
        "            'remover_duplicatas': True,\n",
        "            'tratar_missing': {\n",
        "                'colunas_remover': ['col1'],\n",
        "                'colunas_imputar_media': ['col2'],\n",
        "                'colunas_imputar_mediana': ['col3'],\n",
        "                'colunas_imputar_moda': ['col4']\n",
        "            },\n",
        "            'remover_outliers': {\n",
        "                'metodo': 'iqr',  # 'iqr' ou 'zscore'\n",
        "                'colunas': ['col5']\n",
        "            },\n",
        "            'padronizar_texto': {\n",
        "                'colunas': ['col6'],\n",
        "                'metodo': 'title'  # 'lower', 'upper', 'title'\n",
        "            }\n",
        "        }\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"ETAPA 2: LIMPEZA DE DADOS\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        linhas_antes = len(self.df_processado)\n",
        "        acoes_realizadas = []\n",
        "\n",
        "        # 1. Remover duplicatas\n",
        "        if config.get('remover_duplicatas', False):\n",
        "            duplicatas_antes = self.df_processado.duplicated().sum()\n",
        "            self.df_processado = self.df_processado.drop_duplicates()\n",
        "            duplicatas_removidas = duplicatas_antes\n",
        "            print(f\"✓ Removidas {duplicatas_removidas} linhas duplicadas\")\n",
        "            acoes_realizadas.append(f\"Duplicatas removidas: {duplicatas_removidas}\")\n",
        "\n",
        "        # 2. Tratar valores ausentes\n",
        "        tratar_missing = config.get('tratar_missing', {})\n",
        "\n",
        "        # Remover colunas\n",
        "        cols_remover = tratar_missing.get('colunas_remover', [])\n",
        "        if cols_remover:\n",
        "            self.df_processado = self.df_processado.drop(columns=cols_remover, errors='ignore')\n",
        "            print(f\"✓ Removidas colunas: {cols_remover}\")\n",
        "            acoes_realizadas.append(f\"Colunas removidas: {len(cols_remover)}\")\n",
        "\n",
        "        # Imputar média\n",
        "        cols_media = tratar_missing.get('colunas_imputar_media', [])\n",
        "        for col in cols_media:\n",
        "            if col in self.df_processado.columns:\n",
        "                missing_antes = self.df_processado[col].isnull().sum()\n",
        "                media = self.df_processado[col].mean()\n",
        "                self.df_processado[col].fillna(media, inplace=True)\n",
        "                print(f\"✓ {col}: imputados {missing_antes} valores com média {media:.2f}\")\n",
        "                acoes_realizadas.append(f\"{col}: média\")\n",
        "\n",
        "        # Imputar mediana\n",
        "        cols_mediana = tratar_missing.get('colunas_imputar_mediana', [])\n",
        "        for col in cols_mediana:\n",
        "            if col in self.df_processado.columns:\n",
        "                missing_antes = self.df_processado[col].isnull().sum()\n",
        "                mediana = self.df_processado[col].median()\n",
        "                self.df_processado[col].fillna(mediana, inplace=True)\n",
        "                print(f\"✓ {col}: imputados {missing_antes} valores com mediana {mediana:.2f}\")\n",
        "                acoes_realizadas.append(f\"{col}: mediana\")\n",
        "\n",
        "        # Imputar moda\n",
        "        cols_moda = tratar_missing.get('colunas_imputar_moda', [])\n",
        "        for col in cols_moda:\n",
        "            if col in self.df_processado.columns:\n",
        "                missing_antes = self.df_processado[col].isnull().sum()\n",
        "                moda = self.df_processado[col].mode()[0] if not self.df_processado[col].mode().empty else None\n",
        "                if moda is not None:\n",
        "                    self.df_processado[col].fillna(moda, inplace=True)\n",
        "                    print(f\"✓ {col}: imputados {missing_antes} valores com moda '{moda}'\")\n",
        "                    acoes_realizadas.append(f\"{col}: moda\")\n",
        "\n",
        "        # 3. Remover outliers\n",
        "        remover_outliers = config.get('remover_outliers', {})\n",
        "        if remover_outliers:\n",
        "            metodo = remover_outliers.get('metodo', 'iqr')\n",
        "            colunas = remover_outliers.get('colunas', [])\n",
        "\n",
        "            for col in colunas:\n",
        "                if col not in self.df_processado.columns:\n",
        "                    continue\n",
        "\n",
        "                linhas_antes_outlier = len(self.df_processado)\n",
        "\n",
        "                if metodo == 'iqr':\n",
        "                    Q1 = self.df_processado[col].quantile(0.25)\n",
        "                    Q3 = self.df_processado[col].quantile(0.75)\n",
        "                    IQR = Q3 - Q1\n",
        "                    lower_bound = Q1 - 1.5 * IQR\n",
        "                    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "                    self.df_processado = self.df_processado[\n",
        "                        (self.df_processado[col] >= lower_bound) &\n",
        "                        (self.df_processado[col] <= upper_bound)\n",
        "                    ]\n",
        "\n",
        "                outliers_removidos = linhas_antes_outlier - len(self.df_processado)\n",
        "                if outliers_removidos > 0:\n",
        "                    print(f\"✓ {col}: removidos {outliers_removidos} outliers ({metodo})\")\n",
        "                    acoes_realizadas.append(f\"{col}: {outliers_removidos} outliers\")\n",
        "\n",
        "        # 4. Padronizar texto\n",
        "        padronizar = config.get('padronizar_texto', {})\n",
        "        if padronizar:\n",
        "            colunas = padronizar.get('colunas', [])\n",
        "            metodo = padronizar.get('metodo', 'title')\n",
        "\n",
        "            for col in colunas:\n",
        "                if col in self.df_processado.columns:\n",
        "                    if metodo == 'lower':\n",
        "                        self.df_processado[col] = self.df_processado[col].str.lower()\n",
        "                    elif metodo == 'upper':\n",
        "                        self.df_processado[col] = self.df_processado[col].str.upper()\n",
        "                    elif metodo == 'title':\n",
        "                        self.df_processado[col] = self.df_processado[col].str.title()\n",
        "\n",
        "                    self.df_processado[col] = self.df_processado[col].str.strip()\n",
        "                    print(f\"✓ {col}: texto padronizado ({metodo})\")\n",
        "                    acoes_realizadas.append(f\"{col}: padronizado\")\n",
        "\n",
        "        linhas_depois = len(self.df_processado)\n",
        "        linhas_removidas = linhas_antes - linhas_depois\n",
        "\n",
        "        print(f\"\\n📊 Resumo da Limpeza:\")\n",
        "        print(f\"  Linhas removidas: {linhas_removidas}\")\n",
        "        print(f\"  Linhas restantes: {linhas_depois}\")\n",
        "\n",
        "        self._registrar_etapa(\"Limpeza\", {\n",
        "            'linhas_removidas': int(linhas_removidas),\n",
        "            'acoes': acoes_realizadas\n",
        "        })\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transformar_dados(self, config):\n",
        "        \"\"\"\n",
        "        Aplica transformações nos dados\n",
        "\n",
        "        config = {\n",
        "            'converter_tipos': {\n",
        "                'data_venda': 'datetime'\n",
        "            },\n",
        "            'criar_features_temporais': ['data_venda'],\n",
        "            'normalizar': {\n",
        "                'metodo': 'standardization',  # ou 'minmax'\n",
        "                'colunas': ['preco', 'quantidade']\n",
        "            },\n",
        "            'encoding': {\n",
        "                'onehot': ['categoria'],\n",
        "                'label': ['status']\n",
        "            }\n",
        "        }\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"ETAPA 3: TRANSFORMAÇÃO DE DADOS\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        colunas_antes = len(self.df_processado.columns)\n",
        "        transformacoes = []\n",
        "\n",
        "        # 1. Converter tipos\n",
        "        converter_tipos = config.get('converter_tipos', {})\n",
        "        for col, tipo in converter_tipos.items():\n",
        "            if col in self.df_processado.columns:\n",
        "                try:\n",
        "                    if tipo == 'datetime':\n",
        "                        self.df_processado[col] = pd.to_datetime(self.df_processado[col])\n",
        "                    elif tipo == 'int':\n",
        "                        self.df_processado[col] = pd.to_numeric(self.df_processado[col], errors='coerce').astype('Int64')\n",
        "                    elif tipo == 'float':\n",
        "                        self.df_processado[col] = pd.to_numeric(self.df_processado[col], errors='coerce')\n",
        "\n",
        "                    print(f\"✓ {col}: convertido para {tipo}\")\n",
        "                    transformacoes.append(f\"{col}: {tipo}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"✗ Erro ao converter {col}: {str(e)}\")\n",
        "\n",
        "        # 2. Features temporais\n",
        "        cols_temporais = config.get('criar_features_temporais', [])\n",
        "        for col in cols_temporais:\n",
        "            if col in self.df_processado.columns:\n",
        "                self.df_processado[f'{col}_ano'] = self.df_processado[col].dt.year\n",
        "                self.df_processado[f'{col}_mes'] = self.df_processado[col].dt.month\n",
        "                self.df_processado[f'{col}_dia_semana'] = self.df_processado[col].dt.dayofweek\n",
        "                print(f\"✓ {col}: features temporais criadas (ano, mes, dia_semana)\")\n",
        "                transformacoes.append(f\"{col}: features temporais\")\n",
        "\n",
        "        # 3. Normalização\n",
        "        normalizar_config = config.get('normalizar', {})\n",
        "        if normalizar_config:\n",
        "            metodo = normalizar_config.get('metodo', 'standardization')\n",
        "            colunas = normalizar_config.get('colunas', [])\n",
        "\n",
        "            for col in colunas:\n",
        "                if col in self.df_processado.columns:\n",
        "                    if metodo == 'standardization':\n",
        "                        mean = self.df_processado[col].mean()\n",
        "                        std = self.df_processado[col].std()\n",
        "                        self.df_processado[f'{col}_std'] = (self.df_processado[col] - mean) / std\n",
        "                        print(f\"✓ {col}: standardization aplicado\")\n",
        "                    elif metodo == 'minmax':\n",
        "                        min_val = self.df_processado[col].min()\n",
        "                        max_val = self.df_processado[col].max()\n",
        "                        self.df_processado[f'{col}_minmax'] = (self.df_processado[col] - min_val) / (max_val - min_val)\n",
        "                        print(f\"✓ {col}: min-max scaling aplicado\")\n",
        "\n",
        "                    transformacoes.append(f\"{col}: {metodo}\")\n",
        "\n",
        "        # 4. Encoding\n",
        "        encoding_config = config.get('encoding', {})\n",
        "\n",
        "        # One-Hot Encoding\n",
        "        cols_onehot = encoding_config.get('onehot', [])\n",
        "        for col in cols_onehot:\n",
        "            if col in self.df_processado.columns:\n",
        "                dummies = pd.get_dummies(self.df_processado[col], prefix=col, drop_first=True)\n",
        "                self.df_processado = pd.concat([self.df_processado, dummies], axis=1)\n",
        "                print(f\"✓ {col}: one-hot encoding ({len(dummies.columns)} colunas criadas)\")\n",
        "                transformacoes.append(f\"{col}: one-hot\")\n",
        "\n",
        "        # Label Encoding\n",
        "        cols_label = encoding_config.get('label', [])\n",
        "        for col in cols_label:\n",
        "            if col in self.df_processado.columns:\n",
        "                self.df_processado[f'{col}_label'] = pd.Categorical(self.df_processado[col]).codes\n",
        "                print(f\"✓ {col}: label encoding aplicado\")\n",
        "                transformacoes.append(f\"{col}: label\")\n",
        "\n",
        "        colunas_depois = len(self.df_processado.columns)\n",
        "        colunas_criadas = colunas_depois - colunas_antes\n",
        "\n",
        "        print(f\"\\n📊 Resumo da Transformação:\")\n",
        "        print(f\"  Colunas criadas: {colunas_criadas}\")\n",
        "        print(f\"  Total de colunas: {colunas_depois}\")\n",
        "\n",
        "        self._registrar_etapa(\"Transformação\", {\n",
        "            'colunas_criadas': int(colunas_criadas),\n",
        "            'transformacoes': transformacoes\n",
        "        })\n",
        "\n",
        "        return self\n",
        "\n",
        "    def validar_dados(self, regras=None):\n",
        "        \"\"\"\n",
        "        Valida dados processados\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"ETAPA 4: VALIDAÇÃO DE DADOS\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        validacoes = {\n",
        "            'passou': 0,\n",
        "            'falhou': 0,\n",
        "            'detalhes': []\n",
        "        }\n",
        "\n",
        "        # Validações padrão\n",
        "        # 1. Sem valores ausentes críticos\n",
        "        missing = self.df_processado.isnull().sum().sum()\n",
        "        if missing == 0:\n",
        "            print(\"✓ Nenhum valor ausente\")\n",
        "            validacoes['passou'] += 1\n",
        "        else:\n",
        "            print(f\"⚠ {missing} valores ausentes restantes\")\n",
        "            validacoes['detalhes'].append(f\"{missing} valores ausentes\")\n",
        "\n",
        "        # 2. Sem duplicatas\n",
        "        duplicatas = self.df_processado.duplicated().sum()\n",
        "        if duplicatas == 0:\n",
        "            print(\"✓ Nenhuma duplicata\")\n",
        "            validacoes['passou'] += 1\n",
        "        else:\n",
        "            print(f\"✗ {duplicatas} duplicatas encontradas\")\n",
        "            validacoes['falhou'] += 1\n",
        "            validacoes['detalhes'].append(f\"{duplicatas} duplicatas\")\n",
        "\n",
        "        # 3. Tamanho do dataset\n",
        "        if len(self.df_processado) > 0:\n",
        "            print(f\"✓ Dataset contém {len(self.df_processado)} registros\")\n",
        "            validacoes['passou'] += 1\n",
        "        else:\n",
        "            print(\"✗ Dataset vazio\")\n",
        "            validacoes['falhou'] += 1\n",
        "\n",
        "        # Validações customizadas\n",
        "        if regras:\n",
        "            for regra_nome, regra_func in regras.items():\n",
        "                try:\n",
        "                    resultado = regra_func(self.df_processado)\n",
        "                    if resultado:\n",
        "                        print(f\"✓ {regra_nome}\")\n",
        "                        validacoes['passou'] += 1\n",
        "                    else:\n",
        "                        print(f\"✗ {regra_nome}\")\n",
        "                        validacoes['falhou'] += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"✗ {regra_nome}: erro - {str(e)}\")\n",
        "                    validacoes['falhou'] += 1\n",
        "\n",
        "        print(f\"\\n📊 Resumo da Validação:\")\n",
        "        print(f\"  Passou: {validacoes['passou']}\")\n",
        "        print(f\"  Falhou: {validacoes['falhou']}\")\n",
        "\n",
        "        self._registrar_etapa(\"Validação\", validacoes)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def salvar_dados(self, caminho_saida, formato='csv'):\n",
        "        \"\"\"\n",
        "        Salva dados processados\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"SALVANDO DADOS PROCESSADOS\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        try:\n",
        "            if formato == 'csv':\n",
        "                self.df_processado.to_csv(caminho_saida, index=False)\n",
        "            elif formato == 'parquet':\n",
        "                self.df_processado.to_parquet(caminho_saida, index=False)\n",
        "            elif formato == 'excel':\n",
        "                self.df_processado.to_excel(caminho_saida, index=False)\n",
        "\n",
        "            tamanho_mb = os.path.getsize(caminho_saida) / 1024**2\n",
        "            print(f\"✓ Dados salvos em: {caminho_saida}\")\n",
        "            print(f\"  Formato: {formato}\")\n",
        "            print(f\"  Tamanho: {tamanho_mb:.2f} MB\")\n",
        "\n",
        "            self._registrar_etapa(\"Salvamento\", {\n",
        "                'arquivo': caminho_saida,\n",
        "                'formato': formato,\n",
        "                'tamanho_mb': float(tamanho_mb)\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Erro ao salvar: {str(e)}\")\n",
        "\n",
        "    def gerar_relatorio(self):\n",
        "        \"\"\"\n",
        "        Gera relatório completo do pipeline\n",
        "        \"\"\"\n",
        "        self.tempo_fim = datetime.now()\n",
        "        tempo_total = (self.tempo_fim - self.tempo_inicio).total_seconds()\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"RELATÓRIO FINAL DO PIPELINE\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        print(f\"Pipeline: {self.nome_pipeline}\")\n",
        "        print(f\"Início: {self.tempo_inicio.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(f\"Fim: {self.tempo_fim.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(f\"Duração: {tempo_total:.2f} segundos\")\n",
        "\n",
        "        print(f\"\\n📊 Comparação Antes/Depois:\")\n",
        "        print(f\"  Linhas: {len(self.df_original):,} → {len(self.df_processado):,}\")\n",
        "        print(f\"  Colunas: {len(self.df_original.columns)} → {len(self.df_processado.columns)}\")\n",
        "\n",
        "        mem_antes = self.df_original.memory_usage(deep=True).sum() / 1024**2\n",
        "        mem_depois = self.df_processado.memory_usage(deep=True).sum() / 1024**2\n",
        "        print(f\"  Memória: {mem_antes:.2f} MB → {mem_depois:.2f} MB\")\n",
        "\n",
        "        missing_antes = self.df_original.isnull().sum().sum()\n",
        "        missing_depois = self.df_processado.isnull().sum().sum()\n",
        "        print(f\"  Missing: {missing_antes:,} → {missing_depois:,}\")\n",
        "\n",
        "        print(f\"\\n🔄 Etapas Executadas:\")\n",
        "        for etapa in self.etapas_executadas:\n",
        "            print(f\"  • {etapa['nome']}\")\n",
        "\n",
        "        # Salvar relatório JSON\n",
        "        relatorio = {\n",
        "            'pipeline': self.nome_pipeline,\n",
        "            'inicio': self.tempo_inicio.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'fim': self.tempo_fim.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'duracao_segundos': tempo_total,\n",
        "            'dados_originais': {\n",
        "                'linhas': len(self.df_original),\n",
        "                'colunas': len(self.df_original.columns),\n",
        "                'memoria_mb': mem_antes,\n",
        "                'missing': int(missing_antes)\n",
        "            },\n",
        "            'dados_processados': {\n",
        "                'linhas': len(self.df_processado),\n",
        "                'colunas': len(self.df_processado.columns),\n",
        "                'memoria_mb': mem_depois,\n",
        "                'missing': int(missing_depois)\n",
        "            },\n",
        "            'etapas': self.etapas_executadas\n",
        "        }\n",
        "\n",
        "        with open(f'relatorio_{self.nome_pipeline}.json', 'w', encoding='utf-8') as f:\n",
        "            json.dump(relatorio, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"\\n✓ Relatório JSON salvo: relatorio_{self.nome_pipeline}.json\")\n",
        "\n",
        "        return relatorio\n",
        "\n",
        "    def _registrar_etapa(self, nome, metricas):\n",
        "        \"\"\"Registra etapa executada\"\"\"\n",
        "        self.etapas_executadas.append({\n",
        "            'nome': nome,\n",
        "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'metricas': metricas\n",
        "        })\n",
        "\n",
        "    def obter_dados_processados(self):\n",
        "        \"\"\"Retorna dataframe processado\"\"\"\n",
        "        return self.df_processado\n",
        "\n",
        "\n",
        "# ==================== CONFIGURAÇÕES DE EXEMPLO ====================\n",
        "\n",
        "CONFIG_LIMPEZA_VENDAS = {\n",
        "    'remover_duplicatas': True,\n",
        "    'tratar_missing': {\n",
        "        'colunas_imputar_mediana': ['quantidade'],\n",
        "        'colunas_imputar_media': ['preco_unitario'],\n",
        "        'colunas_imputar_moda': ['status']\n",
        "    },\n",
        "    'remover_outliers': {\n",
        "        'metodo': 'iqr',\n",
        "        'colunas': ['preco_unitario']\n",
        "    },\n",
        "    'padronizar_texto': {\n",
        "        'colunas': ['produto', 'cliente', 'status'],\n",
        "        'metodo': 'title'\n",
        "    }\n",
        "}\n",
        "\n",
        "CONFIG_TRANSFORMACAO_VENDAS = {\n",
        "    'converter_tipos': {\n",
        "        'data_venda': 'datetime',\n",
        "        'quantidade': 'int'\n",
        "    },\n",
        "    'criar_features_temporais': ['data_venda'],\n",
        "    'normalizar': {\n",
        "        'metodo': 'standardization',\n",
        "        'colunas': ['preco_unitario', 'quantidade']\n",
        "    },\n",
        "    'encoding': {\n",
        "        'onehot': ['produto'],\n",
        "        'label': ['status']\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "# ==================== EXEMPLO DE USO ====================\n",
        "\n",
        "def executar_pipeline_vendas():\n",
        "    \"\"\"\n",
        "    Executa pipeline completo para dados de vendas\n",
        "    \"\"\"\n",
        "    # Criar pipeline\n",
        "    pipeline = PipelinePreProcessamento(nome_pipeline=\"Vendas_V1\")\n",
        "\n",
        "    # Executar etapas\n",
        "    (pipeline\n",
        "        .carregar_dados('dados_vendas_raw.csv')\n",
        "        .explorar_dados()\n",
        "        .limpar_dados(CONFIG_LIMPEZA_VENDAS)\n",
        "        .transformar_dados(CONFIG_TRANSFORMACAO_VENDAS)\n",
        "        .validar_dados()\n",
        "        .salvar_dados('dados_vendas_processados.csv', formato='csv')\n",
        "        .gerar_relatorio()\n",
        "    )\n",
        "\n",
        "    return pipeline\n",
        "\n",
        "\n",
        "def executar_pipeline_customizado():\n",
        "    \"\"\"\n",
        "    Exemplo de pipeline com configurações customizadas\n",
        "    \"\"\"\n",
        "    # Configuração customizada\n",
        "    config_limpeza = {\n",
        "        'remover_duplicatas': True,\n",
        "        'tratar_missing': {\n",
        "            'colunas_imputar_mediana': ['valor'],\n",
        "        },\n",
        "        'padronizar_texto': {\n",
        "            'colunas': ['localizacao'],\n",
        "            'metodo': 'title'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    config_transformacao = {\n",
        "        'converter_tipos': {\n",
        "            'timestamp': 'datetime'\n",
        "        },\n",
        "        'criar_features_temporais': ['timestamp']\n",
        "    }\n",
        "\n",
        "    # Regras de validação customizadas\n",
        "    def validar_valores_positivos(df):\n",
        "        return (df['valor'] > 0).all()\n",
        "\n",
        "    def validar_sensores_unicos(df):\n",
        "        return df['sensor_id'].nunique() > 0\n",
        "\n",
        "    regras_validacao = {\n",
        "        'Valores positivos': validar_valores_positivos,\n",
        "        'Sensores únicos': validar_sensores_unicos\n",
        "    }\n",
        "\n",
        "    # Executar pipeline\n",
        "    pipeline = PipelinePreProcessamento(nome_pipeline=\"Sensores_V1\")\n",
        "\n",
        "    (pipeline\n",
        "        .carregar_dados('dados_sensores_raw.csv')\n",
        "        .explorar_dados()\n",
        "        .limpar_dados(config_limpeza)\n",
        "        .transformar_dados(config_transformacao)\n",
        "        .validar_dados(regras_validacao)\n",
        "        .salvar_dados('dados_sensores_processados.csv')\n",
        "        .gerar_relatorio()\n",
        "    )\n",
        "\n",
        "    return pipeline\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\"\"\n",
        "╔══════════════════════════════════════════════════════════════════════╗\n",
        "║                PIPELINE DE PRÉ-PROCESSAMENTO DE DADOS                ║\n",
        "║                          Aula 02 - PÓS-GRADUAÇÃO                     ║\n",
        "╚══════════════════════════════════════════════════════════════════════╝\n",
        "    \"\"\")\n",
        "\n",
        "    # Verificar se arquivos existem\n",
        "    if not os.path.exists('dados_vendas_raw.csv'):\n",
        "        print(\"⚠ Execute primeiro o script 01_geracao_dados_exemplo.py\")\n",
        "        print(\"   para criar os arquivos de dados necessários.\\n\")\n",
        "        exit(1)\n",
        "\n",
        "    print(\"\\n🚀 Executando Pipeline de Vendas...\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    pipeline_vendas = executar_pipeline_vendas()\n",
        "\n",
        "    print(\"\\n\\n\" + \"=\"*70)\n",
        "    print(\"✓ PIPELINE CONCLUÍDO COM SUCESSO!\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\n📁 Arquivos Gerados:\")\n",
        "    print(\"  • dados_vendas_processados.csv\")\n",
        "    print(\"  • relatorio_Vendas_V1.json\")\n",
        "\n",
        "    # Executar pipeline de sensores se arquivo existir\n",
        "    if os.path.exists('dados_sensores_raw.csv'):\n",
        "        print(\"\\n\\n🚀 Executando Pipeline de Sensores...\")\n",
        "        print(\"=\"*70)\n",
        "        pipeline_sensores = executar_pipeline_customizado()\n",
        "        print(\"\\n📁 Arquivos Adicionais Gerados:\")\n",
        "        print(\"  • dados_sensores_processados.csv\")\n",
        "        print(\"  • relatorio_Sensores_V1.json\")\n",
        "\n",
        "    print(\"\\n\\n\" + \"=\"*70)\n",
        "    print(\"💡 DICAS PARA OS ALUNOS:\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\"\"\n",
        "1. Explore os relatórios JSON gerados para entender cada etapa\n",
        "2. Modifique as configurações de limpeza e transformação\n",
        "3. Adicione suas próprias regras de validação\n",
        "4. Teste o pipeline com diferentes datasets\n",
        "5. Compare os dados antes e depois do processamento\n",
        "6. Experimente diferentes métodos de imputação e normalização\n",
        "    \"\"\")\n",
        "\n",
        "    print(\"\\n✓ Exercícios práticos prontos para a aula!\")\n",
        "    print(\"=\"*70)"
      ],
      "metadata": {
        "id": "6M_OgWPjDDOY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}